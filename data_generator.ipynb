{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_bernoulli(seq, prob=0.005):\n",
    "    idx = torch.bernoulli(prob * torch.ones(len(seq))).nonzero().squeeze(dim=1)\n",
    "    s = list(seq)\n",
    "\n",
    "    for i in idx.tolist():\n",
    "        s[i] = \"N\"\n",
    "\n",
    "    return \"\".join(s)\n",
    "\n",
    "def sequences_augmentation(data, level, cat, n):\n",
    "    to_copy = data.loc[data[level] == cat]\n",
    "\n",
    "    new_data = to_copy[0:1]\n",
    "    new_data = new_data.drop(new_data.index[0])\n",
    "\n",
    "    while new_data.shape[0] < n:\n",
    "        qnt = ((n-(new_data.shape[0])) / to_copy.shape[0]).__ceil__()\n",
    "\n",
    "        new_data = pd.concat(([to_copy]*qnt)+[new_data])\n",
    "        new_data[\"truncated_sequence\"] = new_data[\"truncated_sequence\"].apply(augmentation_bernoulli, prob=0.002)\n",
    "        new_data = new_data.drop_duplicates(subset=[\"truncated_sequence\"])\n",
    "    \n",
    "    new_data = new_data[:n-to_copy.shape[0]]\n",
    "    return new_data\n",
    "\n",
    "def data_augmentation(data, level, lower, upper):\n",
    "    class_count = data.groupby(level)[level].count().reset_index(name=\"count\")\n",
    "    \n",
    "    cats = class_count.loc[(class_count[\"count\"] < upper) & (class_count[\"count\"] >= lower)][level].to_list()\n",
    "\n",
    "    clones = sequences_augmentation(data, level, cats[0], upper)\n",
    "    for cat in cats[1:]:\n",
    "        clones = pd.concat([clones, sequences_augmentation(data, level, cat, upper)])\n",
    "\n",
    "    return pd.concat([data, clones])\n",
    "\n",
    "\n",
    "# Load and filter the data from csv\n",
    "def load_data(dataset, level, minimun_entries):\n",
    "    data = dataset.loc[dataset[level].notna()]\n",
    "    data = data.loc[data[\"truncated_sequence\"].str.len() >= 900].sample(frac=1, random_state=42)\n",
    "\n",
    "    # Remove sequences classified in more than one class\n",
    "    tmp = data.groupby(\"truncated_sequence\")[level].nunique().reset_index()\n",
    "    tmp = tmp.loc[tmp[level]>1][\"truncated_sequence\"]\n",
    "    data = data.loc[~data.truncated_sequence.isin(tmp)]\n",
    "\n",
    "    # Remove duplicates on current level\n",
    "    data.drop_duplicates(subset=[level, \"truncated_sequence\"], inplace=True)\n",
    "\n",
    "    # Remove entries from classes with lass than \"minimun_entries\" datapoints\n",
    "    count_classes = data[level].value_counts().reset_index()\n",
    "    selected_classes = count_classes.loc[count_classes[\"count\"] >= minimun_entries]\n",
    "    data = data.loc[data[level].isin(selected_classes[level])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference map for IUPAC sequences encode\n",
    "base_map = {\n",
    "    \"A\":[1.0, 0.0, 0.0, 0.0],\n",
    "    \"T\":[0.0, 1.0, 0.0, 0.0],\n",
    "    \"G\":[0.0, 0.0, 1.0, 0.0],\n",
    "    \"C\":[0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    'W':[0.5, 0.5, 0.0, 0.0],\n",
    "    'S':[0.0, 0.0, 0.5, 0.5],\n",
    "    'M':[0.5, 0.0, 0.0, 0.5],\n",
    "    'K':[0.0, 0.5, 0.5, 0.0],\n",
    "    'R':[0.5, 0.0, 0.5, 0.0],\n",
    "    'Y':[0.0, 0.5, 0.0, 0.5],\n",
    "    \n",
    "    'B':[0.0, 0.3, 0.3, 0.3],\n",
    "    'D':[0.3, 0.3, 0.3, 0.0],\n",
    "    'H':[0.3, 0.3, 0.0, 0.3],\n",
    "    'V':[0.3, 0.0, 0.3, 0.3],\n",
    "\n",
    "    'N':[0.25, 0.25, 0.25, 0.25],\n",
    "}\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    encoded_seq = []\n",
    "\n",
    "    for base in sequence:\n",
    "        encoded_seq.append(base_map[base])\n",
    "    \n",
    "    return torch.tensor(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base dataset\n",
    "csv = pd.read_csv(\"./data/cleaned_sequences.csv\", \n",
    "                  usecols=[\n",
    "                      'domain', \n",
    "                      'supergroup', \n",
    "                      'division', \n",
    "                      'subdivision', \n",
    "                      'class', \n",
    "                      'order', \n",
    "                      'family', \n",
    "                      'genus', \n",
    "                      'species', \n",
    "                      'truncated_sequence'\n",
    "                     ])\n",
    "csv.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to export the generated data\n",
    "base_path = \"./new_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxonomy levels to filter\n",
    "levels = [\"domain\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n",
    "\n",
    "# Format the row to the content format of the taxonomy file\n",
    "def taxonomy_format(row, target_level):\n",
    "    tax = []\n",
    "    for level in levels:\n",
    "        if level in row.index:\n",
    "            tax.append(str(level[0])+\"__\"+(\"\" if pd.isna(row[level]) else row[level]))\n",
    "            if level == target_level:\n",
    "                break\n",
    "    row[\"taxonomy\"] = \"; \".join(tax)\n",
    "    return row\n",
    "\n",
    "# Export data to a taxonomy file\n",
    "def taxonomy_generate(df, target_level, name, path):\n",
    "    tsv = df.apply(taxonomy_format, axis=1, args=(target_level,)).reset_index(names=\"seq_id\")\n",
    "    tsv[[\"seq_id\", \"taxonomy\"]].to_csv(path+\"/\"+name+\"_taxonomy.txt\", sep=\"\\t\", header=False, index=False, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the fasta file with the dataset data\n",
    "def fasta_generate(df, name, path):\n",
    "    with open(path+\"/\"+name+\".fasta\", \"w+\") as fasta:\n",
    "        for index, row in df.iterrows():\n",
    "            fasta.write(\">\"+str(index)+\"\\n\")\n",
    "            fasta.write(row[\"truncated_sequence\"]+\"\\n\")\n",
    "                \n",
    "        fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = 0.10        # Train size\n",
    "k_min = 10          # Minimum n of entries per class\n",
    "k_splits = k_min    # N of clusters for StratifiedSplit with KFold\n",
    "\n",
    "def StratifiedSplit(data, level, rand=42):\n",
    "    _, (X, y) = next(enumerate(StratifiedKFold(n_splits=k_splits, shuffle=True, random_state=rand).split(data.index, data[level])))\n",
    "    return (data.iloc[X], data.iloc[y])\n",
    "\n",
    "def StratifiedSplit2(data, level, rand=42):\n",
    "    return train_test_split(data, test_size=prop, stratify=data[level], random_state=rand)\n",
    "\n",
    "def RandomSplit(data, level=None, rand=42):\n",
    "    test_data = data.sample(frac=prop, random_state=rand)\n",
    "    return (data.drop(test_data.index), test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split functions to be executed\n",
    "splitters = [\n",
    "    # StratifiedSplit, \n",
    "    StratifiedSplit2,\n",
    "    RandomSplit,\n",
    "    ]\n",
    "\n",
    "# Generate and export the files for each of selected level\n",
    "for target_level in [\"class\", \"order\", \"family\", \"genus\", \"species\"]:\n",
    "\n",
    "    # Load data and filter the classes with at least K entries\n",
    "    dataset = load_data(csv, target_level, k_min)\n",
    "    \n",
    "    #Remove subsequent levels\n",
    "    for l in levels[levels.index(target_level)+1:]:\n",
    "        dataset[l] = np.nan\n",
    "    dataset=dataset.dropna(subset=levels[:levels.index(target_level)])\n",
    "    \n",
    "    for randomness in [0, 14, 56, 92, 84, 101, 105, 227]:\n",
    "        for splitter in splitters:\n",
    "            path = base_path+\"/prop_\"+(str(prop).replace(\".\", \"-\"))+\"/min_\"+str(k_splits)+\"/\"+splitter.__name__+\"_\"+str(randomness)+\"/\"+target_level\n",
    "\n",
    "            train_dataset, test_dataset = splitter(dataset, level=target_level, rand=randomness)\n",
    "\n",
    "            print(\"Level: \"+target_level)\n",
    "            print(\"Split: \"+splitter.__name__+\"_\"+str(randomness))\n",
    "            print(\"Train size: \"+str(train_dataset.shape[0]))\n",
    "            print(\"Test size: \"+str(test_dataset.shape[0]))\n",
    "            print(\"\\n\")\n",
    "\n",
    "            # print(\"Original Distribution:\")\n",
    "            # print(dataset[target_level].value_counts(normalize=True))\n",
    "            # print(\"\\nTrain Set Distribution:\")\n",
    "            # print(train_dataset[target_level].value_counts(normalize=True))\n",
    "            # print(\"\\nTest Set Distribution:\")\n",
    "            # print(test_dataset[target_level].value_counts(normalize=True))\n",
    "            # print(\"\\n\")\n",
    "            # break\n",
    "\n",
    "            # Generate files with the current and previous levels\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            train_dataset.to_csv(path+\"/train_dataset.csv\")\n",
    "            test_dataset.to_csv(path+\"/test_dataset.csv\")\n",
    "            \n",
    "            taxonomy_generate(train_dataset, target_level, \"pr2_train\", path)\n",
    "            fasta_generate(train_dataset, \"pr2_train\", path)\n",
    "\n",
    "            taxonomy_generate(test_dataset, target_level, \"pr2_test\", path)\n",
    "            fasta_generate(test_dataset, \"pr2_test\", path)\n",
    "\n",
    "            # Generate files only with the current level \n",
    "            # path = base_path+\"/Isolated\"+splitter.__name__+\"/\"+target_level\n",
    "            # os.makedirs(path, exist_ok=True)\n",
    "            # train_dataset = train_dataset[[target_level, \"truncated_sequence\"]]\n",
    "            # test_dataset = test_dataset[[target_level, \"truncated_sequence\"]]\n",
    "            \n",
    "            # taxonomy_generate(train_dataset, target_level, \"pr2_train\", path)\n",
    "            # fasta_generate(train_dataset, \"pr2_train\", path)\n",
    "\n",
    "            # taxonomy_generate(test_dataset, target_level, \"pr2_test\", path)\n",
    "            # fasta_generate(test_dataset, \"pr2_test\", path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = [\n",
    "    prop_0-1/min_5/RandomSplit_0 prop_0-1/min_5/RandomSplit_14 prop_0-1/min_5/RandomSplit_56 prop_0-1/min_5/RandomSplit_84 prop_0-1/min_5/RandomSplit_92 prop_0-1/min_5/RandomSplit_101 prop_0-1/min_5/RandomSplit_105 prop_0-1/min_5/RandomSplit_227 prop_0-1/min_5/StratifiedSplit2_0 prop_0-1/min_5/StratifiedSplit2_14 prop_0-1/min_5/StratifiedSplit2_56 prop_0-1/min_5/StratifiedSplit2_84 prop_0-1/min_5/StratifiedSplit2_92 prop_0-1/min_5/StratifiedSplit2_101 prop_0-1/min_5/StratifiedSplit2_105 prop_0-1/min_5/StratifiedSplit2_227 prop_0-1/min_10/RandomSplit_0 prop_0-1/min_10/RandomSplit_14 prop_0-1/min_10/RandomSplit_56 prop_0-1/min_10/RandomSplit_84 prop_0-1/min_10/RandomSplit_92 prop_0-1/min_10/RandomSplit_101 prop_0-1/min_10/RandomSplit_105 prop_0-1/min_10/RandomSplit_227 prop_0-1/min_10/StratifiedSplit2_0 prop_0-1/min_10/StratifiedSplit2_14 prop_0-1/min_10/StratifiedSplit2_56 prop_0-1/min_10/StratifiedSplit2_84 prop_0-1/min_10/StratifiedSplit2_92 prop_0-1/min_10/StratifiedSplit2_101 prop_0-1/min_10/StratifiedSplit2_105 prop_0-1/min_10/StratifiedSplit2_227 prop_0-2/min_5/RandomSplit_0 prop_0-2/min_5/RandomSplit_14 prop_0-2/min_5/RandomSplit_56 prop_0-2/min_5/RandomSplit_84 prop_0-2/min_5/RandomSplit_92 prop_0-2/min_5/RandomSplit_101 prop_0-2/min_5/RandomSplit_105 prop_0-2/min_5/RandomSplit_227 prop_0-2/min_5/StratifiedSplit2_0 prop_0-2/min_5/StratifiedSplit2_14 prop_0-2/min_5/StratifiedSplit2_56 prop_0-2/min_5/StratifiedSplit2_84 prop_0-2/min_5/StratifiedSplit2_92 prop_0-2/min_5/StratifiedSplit2_101 prop_0-2/min_5/StratifiedSplit2_105 prop_0-2/min_5/StratifiedSplit2_227 prop_0-2/min_10/RandomSplit_0 prop_0-2/min_10/RandomSplit_14 prop_0-2/min_10/RandomSplit_56 prop_0-2/min_10/RandomSplit_84 prop_0-2/min_10/RandomSplit_92 prop_0-2/min_10/RandomSplit_101 prop_0-2/min_10/RandomSplit_105 prop_0-2/min_10/RandomSplit_227 prop_0-2/min_10/StratifiedSplit2_0 prop_0-2/min_10/StratifiedSplit2_14 prop_0-2/min_10/StratifiedSplit2_56 prop_0-2/min_10/StratifiedSplit2_84 prop_0-2/min_10/StratifiedSplit2_92 prop_0-2/min_10/StratifiedSplit2_101 prop_0-2/min_10/StratifiedSplit2_105 prop_0-2/min_10/StratifiedSplit2_227 prop_0-05/min_5/RandomSplit_0 prop_0-05/min_5/RandomSplit_14 prop_0-05/min_5/RandomSplit_56 prop_0-05/min_5/RandomSplit_84 prop_0-05/min_5/RandomSplit_92 prop_0-05/min_5/RandomSplit_101 prop_0-05/min_5/RandomSplit_105 prop_0-05/min_5/RandomSplit_227 prop_0-05/min_5/StratifiedSplit2_0 prop_0-05/min_5/StratifiedSplit2_14 prop_0-05/min_5/StratifiedSplit2_56 prop_0-05/min_5/StratifiedSplit2_84 prop_0-05/min_5/StratifiedSplit2_92 prop_0-05/min_5/StratifiedSplit2_101 prop_0-05/min_5/StratifiedSplit2_105 prop_0-05/min_5/StratifiedSplit2_227 prop_0-05/min_10/RandomSplit_0 prop_0-05/min_10/RandomSplit_14 prop_0-05/min_10/RandomSplit_56 prop_0-05/min_10/RandomSplit_84 prop_0-05/min_10/RandomSplit_92 prop_0-05/min_10/RandomSplit_101 prop_0-05/min_10/RandomSplit_105 prop_0-05/min_10/RandomSplit_227 prop_0-05/min_10/StratifiedSplit2_0 prop_0-05/min_10/StratifiedSplit2_14 prop_0-05/min_10/StratifiedSplit2_56 prop_0-05/min_10/StratifiedSplit2_84 prop_0-05/min_10/StratifiedSplit2_92 prop_0-05/min_10/StratifiedSplit2_101 prop_0-05/min_10/StratifiedSplit2_105 prop_0-05/min_10/StratifiedSplit2_227\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
