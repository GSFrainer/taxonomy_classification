{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_bernoulli(seq, prob=0.005):\n",
    "    idx = torch.bernoulli(prob * torch.ones(len(seq))).nonzero().squeeze(dim=1)\n",
    "    s = list(seq)\n",
    "\n",
    "    for i in idx.tolist():\n",
    "        s[i] = \"N\"\n",
    "\n",
    "    return \"\".join(s)\n",
    "\n",
    "def sequences_augmentation(data, level, cat, n):\n",
    "    to_copy = data.loc[data[level] == cat]\n",
    "\n",
    "    new_data = to_copy[0:1]\n",
    "    new_data = new_data.drop(new_data.index[0])\n",
    "\n",
    "    while new_data.shape[0] < n:\n",
    "        qnt = ((n-(new_data.shape[0])) / to_copy.shape[0]).__ceil__()\n",
    "\n",
    "        new_data = pd.concat(([to_copy]*qnt)+[new_data])\n",
    "        new_data[\"truncated_sequence\"] = new_data[\"truncated_sequence\"].apply(augmentation_bernoulli, prob=0.002)\n",
    "        new_data = new_data.drop_duplicates(subset=[\"truncated_sequence\"])\n",
    "    \n",
    "    new_data = new_data[:n-to_copy.shape[0]]\n",
    "    return new_data\n",
    "\n",
    "def data_augmentation(data, level, lower, upper):\n",
    "    class_count = data.groupby(level)[level].count().reset_index(name=\"count\")\n",
    "    \n",
    "    cats = class_count.loc[(class_count[\"count\"] < upper) & (class_count[\"count\"] >= lower)][level].to_list()\n",
    "\n",
    "    clones = sequences_augmentation(data, level, cats[0], upper)\n",
    "    for cat in cats[1:]:\n",
    "        clones = pd.concat([clones, sequences_augmentation(data, level, cat, upper)])\n",
    "\n",
    "    return pd.concat([data, clones])\n",
    "\n",
    "\n",
    "# Load and filter the data from csv\n",
    "def load_data(dataset, level, minimun_entries):\n",
    "    data = dataset.loc[dataset[level].notna()]\n",
    "    data = data.loc[data[\"truncated_sequence\"].str.len() >= 900].sample(frac=1, random_state=42)\n",
    "\n",
    "    # Remove sequences classified in more than one class\n",
    "    tmp = data.groupby(\"truncated_sequence\")[level].nunique().reset_index()\n",
    "    tmp = tmp.loc[tmp[level]>1][\"truncated_sequence\"]\n",
    "    data = data.loc[~data.truncated_sequence.isin(tmp)]\n",
    "\n",
    "    # Remove duplicates on current level\n",
    "    data.drop_duplicates(subset=[level, \"truncated_sequence\"], inplace=True)\n",
    "\n",
    "    # Remove entries from classes with lass than \"minimun_entries\" datapoints\n",
    "    count_classes = data[level].value_counts().reset_index()\n",
    "    selected_classes = count_classes.loc[count_classes[\"count\"] >= minimun_entries]\n",
    "    data = data.loc[data[level].isin(selected_classes[level])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference map for IUPAC sequences encode\n",
    "base_map = {\n",
    "    \"A\":[1.0, 0.0, 0.0, 0.0],\n",
    "    \"T\":[0.0, 1.0, 0.0, 0.0],\n",
    "    \"G\":[0.0, 0.0, 1.0, 0.0],\n",
    "    \"C\":[0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    'W':[0.5, 0.5, 0.0, 0.0],\n",
    "    'S':[0.0, 0.0, 0.5, 0.5],\n",
    "    'M':[0.5, 0.0, 0.0, 0.5],\n",
    "    'K':[0.0, 0.5, 0.5, 0.0],\n",
    "    'R':[0.5, 0.0, 0.5, 0.0],\n",
    "    'Y':[0.0, 0.5, 0.0, 0.5],\n",
    "    \n",
    "    'B':[0.0, 0.3, 0.3, 0.3],\n",
    "    'D':[0.3, 0.3, 0.3, 0.0],\n",
    "    'H':[0.3, 0.3, 0.0, 0.3],\n",
    "    'V':[0.3, 0.0, 0.3, 0.3],\n",
    "\n",
    "    'N':[0.25, 0.25, 0.25, 0.25],\n",
    "}\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    encoded_seq = []\n",
    "\n",
    "    for base in sequence:\n",
    "        encoded_seq.append(base_map[base])\n",
    "    \n",
    "    return torch.tensor(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>supergroup</th>\n",
       "      <th>division</th>\n",
       "      <th>subdivision</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "      <th>truncated_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eukaryota</td>\n",
       "      <td>Amoebozoa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UI13E03-lineage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GATAAGCCATGCAAATTTAAATTTAAGCCGGTTTCGGCGAAATTGT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      domain supergroup division subdivision class            order family  \\\n",
       "0  Eukaryota  Amoebozoa      NaN         NaN   NaN  UI13E03-lineage    NaN   \n",
       "\n",
       "  genus species                                 truncated_sequence  \n",
       "0   NaN     NaN  GATAAGCCATGCAAATTTAAATTTAAGCCGGTTTCGGCGAAATTGT...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the base dataset\n",
    "csv = pd.read_csv(\"../data/cleaned_sequences.csv\", \n",
    "                  usecols=[\n",
    "                      'domain', \n",
    "                      'supergroup', \n",
    "                      'division', \n",
    "                      'subdivision', \n",
    "                      'class', \n",
    "                      'order', \n",
    "                      'family', \n",
    "                      'genus', \n",
    "                      'species', \n",
    "                      'truncated_sequence'\n",
    "                     ])\n",
    "csv.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path to export the generated data\n",
    "base_path = \"../new_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxonomy levels to filter\n",
    "levels = [\"domain\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n",
    "\n",
    "# Format the row to the content format of the taxonomy file\n",
    "def taxonomy_format(row, target_level):\n",
    "    tax = []\n",
    "    for level in levels:\n",
    "        if level in row.index:\n",
    "            tax.append(str(level[0])+\"__\"+(\"\" if pd.isna(row[level]) else row[level]))\n",
    "            if level == target_level:\n",
    "                break\n",
    "    row[\"taxonomy\"] = \"; \".join(tax)\n",
    "    return row\n",
    "\n",
    "# Export data to a taxonomy file\n",
    "def taxonomy_generate(df, target_level, name, path):\n",
    "    tsv = df.apply(taxonomy_format, axis=1, args=(target_level,)).reset_index(names=\"seq_id\")\n",
    "    tsv[[\"seq_id\", \"taxonomy\"]].to_csv(path+\"/\"+name+\"_taxonomy.txt\", sep=\"\\t\", header=False, index=False, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the fasta file with the dataset data\n",
    "def fasta_generate(df, name, path):\n",
    "    with open(path+\"/\"+name+\".fasta\", \"w+\") as fasta:\n",
    "        for index, row in df.iterrows():\n",
    "            fasta.write(\">\"+str(index)+\"\\n\")\n",
    "            fasta.write(row[\"truncated_sequence\"]+\"\\n\")\n",
    "                \n",
    "        fasta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StratifiedSplit(data, level):\n",
    "    _, (X, y) = next(enumerate(StratifiedKFold(n_splits=10, shuffle=True, random_state=42).split(data.index, data[level])))\n",
    "    return (data.iloc[X], data.iloc[y])\n",
    "\n",
    "def RandomSplit(data, level=None):\n",
    "    test_data = data.sample(frac=0.1, random_state=42)\n",
    "    return (data.drop(test_data.index), test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level: class\n",
      "Split: StratifiedSplit\n",
      "Train size: 111138\n",
      "Test size: 12349\n",
      "\n",
      "\n",
      "Level: class\n",
      "Split: RandomSplit\n",
      "Train size: 111138\n",
      "Test size: 12349\n",
      "\n",
      "\n",
      "Level: order\n",
      "Split: StratifiedSplit\n",
      "Train size: 90506\n",
      "Test size: 10057\n",
      "\n",
      "\n",
      "Level: order\n",
      "Split: RandomSplit\n",
      "Train size: 90507\n",
      "Test size: 10056\n",
      "\n",
      "\n",
      "Level: family\n",
      "Split: StratifiedSplit\n",
      "Train size: 73745\n",
      "Test size: 8194\n",
      "\n",
      "\n",
      "Level: family\n",
      "Split: RandomSplit\n",
      "Train size: 73745\n",
      "Test size: 8194\n",
      "\n",
      "\n",
      "Level: genus\n",
      "Split: StratifiedSplit\n",
      "Train size: 37514\n",
      "Test size: 4169\n",
      "\n",
      "\n",
      "Level: genus\n",
      "Split: RandomSplit\n",
      "Train size: 37515\n",
      "Test size: 4168\n",
      "\n",
      "\n",
      "Level: species\n",
      "Split: StratifiedSplit\n",
      "Train size: 10500\n",
      "Test size: 1167\n",
      "\n",
      "\n",
      "Level: species\n",
      "Split: RandomSplit\n",
      "Train size: 10500\n",
      "Test size: 1167\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split functions to be executed\n",
    "splitters = [StratifiedSplit, RandomSplit]\n",
    "\n",
    "# Generate and export the files for each of selected level\n",
    "for target_level in [\"class\", \"order\", \"family\", \"genus\", \"species\"]:\n",
    "\n",
    "    # Load data and filter the classes with at least 10 entries\n",
    "    dataset = load_data(csv, target_level, 10)\n",
    "    \n",
    "    #Remove subsequent levels\n",
    "    for l in levels[levels.index(target_level)+1:]:\n",
    "        dataset[l] = np.nan\n",
    "    dataset=dataset.dropna(subset=levels[:levels.index(target_level)])\n",
    "    \n",
    "    for splitter in splitters:\n",
    "        path = base_path+\"/\"+splitter.__name__+\"/\"+target_level\n",
    "        train_dataset, test_dataset = splitter(dataset, target_level)\n",
    "\n",
    "        print(\"Level: \"+target_level)\n",
    "        print(\"Split: \"+splitter.__name__)\n",
    "        print(\"Train size: \"+str(train_dataset.shape[0]))\n",
    "        print(\"Test size: \"+str(test_dataset.shape[0]))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # Generate files with the current and previous levels\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        train_dataset.to_csv(path+\"/train_dataset.csv\")\n",
    "        test_dataset.to_csv(path+\"/test_dataset.csv\")\n",
    "        \n",
    "        taxonomy_generate(train_dataset, target_level, \"pr2_train\", path)\n",
    "        fasta_generate(train_dataset, \"pr2_train\", path)\n",
    "\n",
    "        taxonomy_generate(test_dataset, target_level, \"pr2_test\", path)\n",
    "        fasta_generate(test_dataset, \"pr2_test\", path)\n",
    "\n",
    "        # Generate files only with the current level \n",
    "        path = base_path+\"/Isolated\"+splitter.__name__+\"/\"+target_level\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        train_dataset = train_dataset[[target_level, \"truncated_sequence\"]]\n",
    "        test_dataset = test_dataset[[target_level, \"truncated_sequence\"]]\n",
    "        \n",
    "        taxonomy_generate(train_dataset, target_level, \"pr2_train\", path)\n",
    "        fasta_generate(train_dataset, \"pr2_train\", path)\n",
    "\n",
    "        taxonomy_generate(test_dataset, target_level, \"pr2_test\", path)\n",
    "        fasta_generate(test_dataset, \"pr2_test\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gustavo_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
