{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_map = {\n",
    "    \"A\":[1.0, 0.0, 0.0, 0.0],\n",
    "    \"T\":[0.0, 1.0, 0.0, 0.0],\n",
    "    \"G\":[0.0, 0.0, 1.0, 0.0],\n",
    "    \"C\":[0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    'W':[0.5, 0.5, 0.0, 0.0],\n",
    "    'S':[0.0, 0.0, 0.5, 0.5],\n",
    "    'M':[0.5, 0.0, 0.0, 0.5],\n",
    "    'K':[0.0, 0.5, 0.5, 0.0],\n",
    "    'R':[0.5, 0.0, 0.5, 0.0],\n",
    "    'Y':[0.0, 0.5, 0.0, 0.5],\n",
    "    \n",
    "    'B':[0.0, 0.3, 0.3, 0.3],\n",
    "    'D':[0.3, 0.3, 0.3, 0.0],\n",
    "    'H':[0.3, 0.3, 0.0, 0.3],\n",
    "    'V':[0.3, 0.0, 0.3, 0.3],\n",
    "\n",
    "    'N':[0.25, 0.25, 0.25, 0.25],\n",
    "}\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    encoded_seq = []\n",
    "\n",
    "    for base in sequence:\n",
    "        encoded_seq.append(base_map[base])\n",
    "    \n",
    "    return torch.tensor(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\"domain\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n",
    "\n",
    "class MultilevelSequenceDataset(Dataset):\n",
    "    def __init__(self, train, test, level):\n",
    "\n",
    "        self.classes = pd.concat([train[level], test[level]]).unique().tolist()\n",
    "        self.classes.sort()\n",
    "        self.level = level\n",
    "\n",
    "        self.labels = train[level]\n",
    "        self.encoded_labels = MultilevelSequenceDataset.__encoded_labels__(self.classes, self.labels)\n",
    "        self.sequences = MultilevelSequenceDataset.__sequences__(train)        \n",
    "\n",
    "        self.previous_level = levels[levels.index(level)-1]\n",
    "        self.previous_classes = pd.concat([train[self.previous_level], test[self.previous_level]]).unique().tolist()\n",
    "        self.previous_classes.sort()\n",
    "        self.previous_encoded_labels = MultilevelSequenceDataset.__encoded_labels__(self.previous_classes, train[self.previous_level])\n",
    "\n",
    "\n",
    "        self.test = MultilevelSequenceDatasetTest(\n",
    "            labels = test[level],\n",
    "            classes = self.classes,\n",
    "            encoded_labels = MultilevelSequenceDataset.__encoded_labels__(self.classes, test[level]),\n",
    "            sequences = MultilevelSequenceDataset.__sequences__(test),\n",
    "            previous_classes = self.previous_classes,\n",
    "            previous_encoded_labels = MultilevelSequenceDataset.__encoded_labels__(self.previous_classes, test[self.previous_level]) ,\n",
    "            previous_level = self.previous_level\n",
    "            )\n",
    "\n",
    "    def __encoded_labels__(classes, labels):\n",
    "        return torch.nn.functional.one_hot(torch.tensor([classes.index(l) for l in labels]), len(classes)).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    def __sequences__(ds):\n",
    "        sequences = []\n",
    "        for _, row in ds.iterrows():\n",
    "            sequences.append(encode_sequence(row[\"truncated_sequence\"]))        \n",
    "        return torch.stack(sequences, dim=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return   self.sequences[idx], self.previous_encoded_labels[idx], self.encoded_labels[idx]\n",
    "    \n",
    "    def __getitems__(self, ids):\n",
    "        idx = torch.tensor(ids, device=torch.device('cuda:0'))\n",
    "        return   list(zip(torch.index_select(self.sequences, 0, idx), torch.index_select(self.previous_encoded_labels, 0, idx), torch.index_select(self.encoded_labels, 0, idx)))\n",
    "    \n",
    "    def get_test(self):\n",
    "        return self.test\n",
    "\n",
    "class MultilevelSequenceDatasetTest(MultilevelSequenceDataset):    \n",
    "    def __init__(self, labels, classes, encoded_labels, sequences, previous_level, previous_classes, previous_encoded_labels ):\n",
    "        self.labels = labels\n",
    "        self.classes = classes\n",
    "        self.encoded_labels = encoded_labels\n",
    "        self.sequences = sequences\n",
    "\n",
    "        self.previous_level = previous_level \n",
    "        self.previous_classes = previous_classes\n",
    "        self.previous_encoded_labels = previous_encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaders_generator(ds_train, ds_test, bs = 128):\n",
    "    train_loader = DataLoader(ds_train, batch_size=bs, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "    test_loader = DataLoader(ds_test, batch_size=bs, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrada: \n",
    "    sequencia (tamanho: 900)\n",
    "    OneHot do nível anterior (tamanho: n_classes) \n",
    "\n",
    "\n",
    "\n",
    "CNN na sequencia\n",
    "flatten na saída da CNN\n",
    "Concatena com o OneHot anterior\n",
    "\n",
    "\n",
    "Primeiro Nível roda com o modelo Simplest\n",
    "Níveis seguintes rodam com o modelo MultilevelSimplest\n",
    "\n",
    "\n",
    "Obs:\n",
    "- Usar o script de treino do batch (teve alterações)\n",
    "- Qual o impacto de usar softmax como saída do modelo? É melhor usar depois do retorno?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilevelSimplestCNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses, nPreviousClasses):\n",
    "        super(MultilevelSimplestCNNClassifier, self).__init__()\n",
    "\n",
    "        print(\"nClasses: \"+str(nClasses))\n",
    "        print(\"nPreviousClasses: \"+str(nPreviousClasses))\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "\n",
    "        self.padding2 = nn.CircularPad1d((1,2))\n",
    "        self.conv2 = nn.Conv1d(8, 32, kernel_size=4)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.padding3 = nn.CircularPad1d((1,2))\n",
    "        self.conv3 = nn.Conv1d(32, 128, kernel_size=4)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28800, 28800*2)\n",
    "        self.linear2 = nn.Linear((28800*2)+nPreviousClasses, nClasses)\n",
    "    \n",
    "    def forward(self, x, px):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.adAvgPool1(x)\n",
    "\n",
    "        \n",
    "        x = self.conv2(self.padding2(x))\n",
    "        x = self.adAvgPool2(x)\n",
    "\n",
    "        \n",
    "        x = self.conv3(self.padding3(x))\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        \n",
    "        x = torch.cat([x, px], dim=1)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_levels_ = [\n",
    "    # \"class\", \n",
    "    \"order\", \n",
    "    # \"family\", \n",
    "    # \"genus\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch_sizes_ = [\n",
    "    # 64,\n",
    "    # 128,\n",
    "    # 256,\n",
    "    # 512,\n",
    "    # 2048,\n",
    "    \"dynamic\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_epochs_ = [\n",
    "    # 2,\n",
    "    # 5,\n",
    "    # 50,\n",
    "    # 100,\n",
    "    200,\n",
    "    # 500\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_models_list_ = [\n",
    "    MultilevelSimplestCNNClassifier,\n",
    "    # SimplestCNNClassifier,\n",
    "    # SimpleCNNClassifier,\n",
    "    # SimpleCNNWithDropoutClassifier,\n",
    "    # UnetBasedCNNClassifier,\n",
    "    # UnetBasedCNNWithDropoutClassifier,\n",
    "    # UnetBasedCNNWithDilationClassifier,\n",
    "    # UnetBasedCNNWithDropoutAndDilationClassifier,\n",
    "    # BaseCNNClassifier, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_loss_functions_ = {\n",
    "    \"CrossEntropyLoss\":{\n",
    "        \"function\":nn.CrossEntropyLoss,\n",
    "        \"params\":{},\n",
    "        \"function_params\":{}\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_learning_rates_ = [\n",
    "    # 1e-2,\n",
    "    # 5e-2,\n",
    "    # 1e-3,\n",
    "    # 5e-3,\n",
    "    # 1e-4,\n",
    "    5e-4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_optimizers_ = [\n",
    "    {\n",
    "        \"optim\":torch.optim.AdamW,\n",
    "        \"params\":{\n",
    "            \"weight_decay\":1e-2,\n",
    "            \"amsgrad\":True\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparams = {\n",
    "    \"levels\": _levels_,\n",
    "    \"batch_size\": _batch_sizes_,\n",
    "    \"epochs\": _epochs_,\n",
    "    \"model\": _models_list_,\n",
    "    \"loss_function\": _loss_functions_,\n",
    "    \"learning_rate\": _learning_rates_,\n",
    "    \"optimizer\": _optimizers_    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'levels': ['order'],\n",
       " 'batch_size': ['dynamic'],\n",
       " 'epochs': [200],\n",
       " 'model': [__main__.MultilevelSimplestCNNClassifier],\n",
       " 'loss_function': {'CrossEntropyLoss': {'function': torch.nn.modules.loss.CrossEntropyLoss,\n",
       "   'params': {},\n",
       "   'function_params': {}}},\n",
       " 'learning_rate': [0.0005],\n",
       " 'optimizer': [{'optim': torch.optim.adamw.AdamW,\n",
       "   'params': {'weight_decay': 0.01, 'amsgrad': True}}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Test(\n",
    "        model, \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        epochs, \n",
    "        learning_rate, \n",
    "        batch_size, \n",
    "        train_data,\n",
    "        test_data,\n",
    "        id=\"\", \n",
    "        ):\n",
    "    \n",
    "    print(\"Model: \\t\\t\\t\"+(model._get_name() if not model._get_name() == \"OptimizedModule\" else model.__dict__[\"_modules\"][\"_orig_mod\"].__class__.__name__))\n",
    "    print(\"  Loss Func.: \\t\\t\"+loss_fn._get_name())\n",
    "    print(\"  Optimizer: \\t\\t\"+type(optimizer).__name__)\n",
    "    print(\"  Epochs: \\t\\t\"+str(epochs))\n",
    "    print(\"  Learning Rate: \\t\"+str(learning_rate))\n",
    "\n",
    "    print(\"\\nModel Arch: \")\n",
    "    print(str(model))\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "    epochs_results = []\n",
    "    current = {\n",
    "        \"model\":(model._get_name() if not model._get_name() == \"OptimizedModule\" else model.__dict__[\"_modules\"][\"_orig_mod\"].__class__.__name__),\n",
    "        \"loss_function\":loss_fn._get_name(),\n",
    "        \"epoch\":None,\n",
    "        \"learning_rate\":learning_rate,\n",
    "        \"batch_size\":None,\n",
    "        \"train_size\":None,\n",
    "        \"test_size\":None,\n",
    "        \"optimizer\":type(optimizer).__name__,\n",
    "        \"train_acc\":None,\n",
    "        \"train_loss\":None,\n",
    "        \"test_acc\":None,\n",
    "        \"test_loss\":None,\n",
    "    }\n",
    "\n",
    "    if batch_size == \"dynamic\":\n",
    "        bss = [1000, 500, 250, 250, 500, 1000, 1000, 1000]\n",
    "    else:\n",
    "        bss = [batch_size]\n",
    "    if len(bss) > epochs:\n",
    "        bss = bss[0:epochs]\n",
    "    print(\"Batch Sizes List: \"+str(bss))\n",
    "    batch_lim = int(epochs/len(bss))\n",
    "    \n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    best = {\n",
    "        \"epoch\":0,\n",
    "        \"train_acc\":0,\n",
    "        \"train_loss\":10000000,\n",
    "        \"test_acc\":0,\n",
    "        \"test_loss\":10000000,\n",
    "    }\n",
    "    \n",
    "    train_loader = None\n",
    "    test_loader = None\n",
    "    \n",
    "    # Epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "\n",
    "        if epoch%batch_lim == 0 and len(bss) > 0:\n",
    "            if train_loader:\n",
    "                del train_loader\n",
    "            if test_loader:\n",
    "                del test_loader\n",
    "\n",
    "            batch_size = bss.pop(0)\n",
    "            train_loader, test_loader = loaders_generator(train_data, test_data, batch_size)\n",
    "\n",
    "        print(\"Batch Size: \"+str(batch_size))\n",
    "\n",
    "        # Train --------------------------------------------------------------------------\n",
    "        # if torch.cuda.get_device_capability() < (7, 0):\n",
    "        #     print(\"Exiting because torch.compile is not supported on this device.\")\n",
    "        #     import sys\n",
    "        #     sys.exit(0)\n",
    "                    \n",
    "        #@torch.compile(fullgraph=False)\n",
    "        fn = torch.compile(optimizer.step)\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        for batch, (X, prevy, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute prediction and loss\n",
    "            pred = model(X, prevy)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            fn()\n",
    "\n",
    "            # Update results\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "        # Train results\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader.dataset)\n",
    "        print(f\"Train: \\n Accuracy: {(100*train_acc):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "        # Test\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, prevy, y in test_loader:\n",
    "                pred = model(X, prevy)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                test_acc += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "        # Test results\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc /= len(test_loader.dataset)\n",
    "        print(f\"Test: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "        # Update Results\n",
    "        if best[\"test_acc\"] < test_acc or (best[\"test_acc\"] == test_acc and best[\"train_acc\"] < train_acc):\n",
    "            best[\"epoch\"] = epoch+1\n",
    "            best[\"test_acc\"] = test_acc\n",
    "            best[\"test_loss\"] = test_loss\n",
    "            best[\"train_acc\"] = train_acc\n",
    "            best[\"train_loss\"] = train_loss\n",
    "        \n",
    "        current[\"epoch\"] = epoch+1\n",
    "        current[\"batch_size\"] = batch_size\n",
    "        current[\"train_size\"] = train_loader.dataset.__len__()\n",
    "        current[\"test_size\"] = test_loader.dataset.__len__()\n",
    "        current[\"train_acc\"] = train_acc\n",
    "        current[\"train_loss\"] = train_loss\n",
    "        current[\"test_acc\"] = test_acc\n",
    "        current[\"test_loss\"] = test_loss\n",
    "\n",
    "    \n",
    "        epochs_results.append(current.copy())\n",
    "\n",
    "    # pd.DataFrame(epochs_results).to_csv(\"./results/epochs/\"+str(id)+\"__\"+current[\"model\"]+\"_train_test.csv\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Best Epoch:{best['epoch']} \\n\\tAccuracy: {(100*best['test_acc']):>0.1f}%, Avg loss: {best['test_loss']:>8f} \\n\")\n",
    "    print(\"Train and Test execution time: \"+str(format(time.time()-t_start, '.4f'))+\"s\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time ID: 1731818850\n",
      "order\n",
      "(90507, 11)\n",
      "(10056, 11)\n",
      "nClasses: 293\n",
      "nPreviousClasses: 98\n",
      "Model: \t\t\tMultilevelSimplestCNNClassifier\n",
      "  Loss Func.: \t\tCrossEntropyLoss\n",
      "  Optimizer: \t\tAdamW\n",
      "  Epochs: \t\t200\n",
      "  Learning Rate: \t0.0005\n",
      "\n",
      "Model Arch: \n",
      "OptimizedModule(\n",
      "  (_orig_mod): MultilevelSimplestCNNClassifier(\n",
      "    (padding1): CircularPad1d((1, 2))\n",
      "    (conv1): Conv1d(4, 8, kernel_size=(4,), stride=(1,))\n",
      "    (adAvgPool1): AdaptiveAvgPool1d(output_size=450)\n",
      "    (padding2): CircularPad1d((1, 2))\n",
      "    (conv2): Conv1d(8, 32, kernel_size=(4,), stride=(1,))\n",
      "    (adAvgPool2): AdaptiveAvgPool1d(output_size=225)\n",
      "    (padding3): CircularPad1d((1, 2))\n",
      "    (conv3): Conv1d(32, 128, kernel_size=(4,), stride=(1,))\n",
      "    (adAvgPool3): AdaptiveAvgPool1d(output_size=225)\n",
      "    (act4): ReLU()\n",
      "    (linear1): Linear(in_features=28800, out_features=57600, bias=True)\n",
      "    (linear2): Linear(in_features=57698, out_features=293, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Batch Sizes List: [1000, 500, 250, 250, 500, 1000, 1000, 1000]\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch Size: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/stark/Models/.conda/envs/gustavo_master/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:150: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W1117 01:49:14.931000 131157631936320 torch/_logging/_internal.py:1034] [1/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      " Accuracy: 42.2%, Avg loss: 3.753450 \n",
      "\n",
      "Test: \n",
      " Accuracy: 70.4%, Avg loss: 1.397917 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 80.0%, Avg loss: 0.894259 \n",
      "\n",
      "Test: \n",
      " Accuracy: 84.9%, Avg loss: 0.700706 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 90.2%, Avg loss: 0.395329 \n",
      "\n",
      "Test: \n",
      " Accuracy: 88.1%, Avg loss: 0.549935 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 95.5%, Avg loss: 0.169951 \n",
      "\n",
      "Test: \n",
      " Accuracy: 89.4%, Avg loss: 0.522351 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 97.6%, Avg loss: 0.090977 \n",
      "\n",
      "Test: \n",
      " Accuracy: 90.1%, Avg loss: 0.516815 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 98.4%, Avg loss: 0.061119 \n",
      "\n",
      "Test: \n",
      " Accuracy: 90.2%, Avg loss: 0.540080 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 98.8%, Avg loss: 0.046574 \n",
      "\n",
      "Test: \n",
      " Accuracy: 90.9%, Avg loss: 0.491783 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.1%, Avg loss: 0.036189 \n",
      "\n",
      "Test: \n",
      " Accuracy: 90.8%, Avg loss: 0.560859 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.3%, Avg loss: 0.031048 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.1%, Avg loss: 0.557985 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.4%, Avg loss: 0.025822 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.3%, Avg loss: 0.507161 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.4%, Avg loss: 0.024492 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.2%, Avg loss: 0.508186 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.5%, Avg loss: 0.020624 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.2%, Avg loss: 0.547605 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.6%, Avg loss: 0.018216 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.0%, Avg loss: 0.592232 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.6%, Avg loss: 0.017161 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.4%, Avg loss: 0.513676 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.7%, Avg loss: 0.013905 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.2%, Avg loss: 0.558043 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.6%, Avg loss: 0.014942 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.2%, Avg loss: 0.529512 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.7%, Avg loss: 0.011908 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.3%, Avg loss: 0.523358 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.8%, Avg loss: 0.010816 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.4%, Avg loss: 0.623131 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.8%, Avg loss: 0.010768 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.515257 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.8%, Avg loss: 0.007617 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.4%, Avg loss: 0.594302 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.8%, Avg loss: 0.008264 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.7%, Avg loss: 0.564516 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.8%, Avg loss: 0.007100 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.609891 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.8%, Avg loss: 0.008413 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.546312 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.006380 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.6%, Avg loss: 0.551474 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Batch Size: 1000\n",
      "Train: \n",
      " Accuracy: 99.8%, Avg loss: 0.008672 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.6%, Avg loss: 0.590246 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 98.8%, Avg loss: 0.055300 \n",
      "\n",
      "Test: \n",
      " Accuracy: 85.8%, Avg loss: 0.797583 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 93.3%, Avg loss: 0.266438 \n",
      "\n",
      "Test: \n",
      " Accuracy: 88.5%, Avg loss: 0.642516 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 97.5%, Avg loss: 0.108827 \n",
      "\n",
      "Test: \n",
      " Accuracy: 90.4%, Avg loss: 0.557481 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.3%, Avg loss: 0.043431 \n",
      "\n",
      "Test: \n",
      " Accuracy: 90.7%, Avg loss: 0.489157 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.3%, Avg loss: 0.044071 \n",
      "\n",
      "Test: \n",
      " Accuracy: 88.9%, Avg loss: 0.619186 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.3%, Avg loss: 0.036386 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.1%, Avg loss: 0.591393 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.7%, Avg loss: 0.014855 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.2%, Avg loss: 0.556935 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.8%, Avg loss: 0.009849 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.3%, Avg loss: 0.589729 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.006252 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.575557 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.005278 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.7%, Avg loss: 0.569387 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.005645 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.6%, Avg loss: 0.538826 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.004829 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.578395 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.004451 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.598195 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.004268 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.6%, Avg loss: 0.564173 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.003779 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.6%, Avg loss: 0.574832 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.003733 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.6%, Avg loss: 0.581999 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.003526 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.4%, Avg loss: 0.564762 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.003692 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.651790 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.003088 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.585284 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.003270 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.5%, Avg loss: 0.591796 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.003080 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.7%, Avg loss: 0.576305 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.002904 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.4%, Avg loss: 0.626130 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.002920 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.7%, Avg loss: 0.575292 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.002372 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.6%, Avg loss: 0.588001 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Batch Size: 500\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.002702 \n",
      "\n",
      "Test: \n",
      " Accuracy: 91.6%, Avg loss: 0.611706 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Batch Size: 250\n",
      "Train: \n",
      " Accuracy: 99.9%, Avg loss: 0.006912 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 83\u001b[0m\n\u001b[1;32m     79\u001b[0m _optimizer_ \u001b[38;5;241m=\u001b[39m optim(_model_\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptim_params)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# ---- Run ----\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mTrain_Test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_model_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_lossfunction_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_optimizer_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_id\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     96\u001b[0m current[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[15], line 119\u001b[0m, in \u001b[0;36mTrain_Test\u001b[0;34m(model, loss_fn, optimizer, epochs, learning_rate, batch_size, train_data, test_data, id)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, prevy, y \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m    118\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(X, prevy)\n\u001b[0;32m--> 119\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m         test_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m y\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Test results\u001b[39;00m\n",
      "File \u001b[0;32m/media/stark/Models/.conda/envs/gustavo_master/lib/python3.12/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "_model_ = None\n",
    "_lossfunction_ = None\n",
    "_optimizer_ = None\n",
    "\n",
    "def clear():\n",
    "    global _model_, _lossfunction_, _optimizer_\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    torch.compiler.reset()\n",
    "    torch._dynamo.reset()\n",
    "\n",
    "    if _model_:\n",
    "        del _model_\n",
    "        _model_ = None\n",
    "    if _lossfunction_:\n",
    "        del _lossfunction_\n",
    "        _lossfunction_ = None\n",
    "    if _optimizer_:\n",
    "        del _optimizer_\n",
    "        _optimizer_ = None\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "results = []\n",
    "current = {}\n",
    "\n",
    "id = 0\n",
    "time_id = str(int(time.time()))\n",
    "print(\"Time ID: \"+str(time_id))\n",
    "\n",
    "for level in hiperparams[\"levels\"]:\n",
    "    clear()\n",
    "\n",
    "    train_data = pd.read_csv(\"../new_data/\"+level+\"/train_dataset.csv\")\n",
    "    test_data = pd.read_csv(\"../new_data/\"+level+\"/test_dataset.csv\")\n",
    "    print(level)\n",
    "    print(train_data.shape)\n",
    "    print(test_data.shape)\n",
    "\n",
    "    dataset = MultilevelSequenceDataset(\n",
    "        train=train_data, \n",
    "        test=test_data, \n",
    "        level=level)\n",
    "\n",
    "\n",
    "    for batch_size in hiperparams[\"batch_size\"]:\n",
    "        for epochs in hiperparams[\"epochs\"]:\n",
    "            for model in hiperparams[\"model\"]:\n",
    "                for loss_function_name, loss_function in hiperparams[\"loss_function\"].items():\n",
    "                    for learning_rate in hiperparams[\"learning_rate\"]:\n",
    "                        for optimizer in hiperparams[\"optimizer\"]:\n",
    "                            \n",
    "                            optim = optimizer[\"optim\"]\n",
    "                            optim_params = optimizer[\"params\"] if \"params\" in optimizer.keys() else {}\n",
    "\n",
    "                            current = {\n",
    "                                    \"id\": id,\n",
    "                                    \"start_time\":time.time(),\n",
    "                                    \"end_time\": None,\n",
    "                                    \"level\": level,\n",
    "                                    \"batch_size\": batch_size,\n",
    "                                    \"epochs\": epochs,\n",
    "                                    \"model\": model.__name__,\n",
    "                                    \"loss_function\": loss_function_name+\" (\"+str(loss_function[\"function\"])+\")\",\n",
    "                                    \"learning_rate\": learning_rate,\n",
    "                                    \"optimizer\": optim.__name__+\" (params: \"+str(optim_params)+\")\",\n",
    "                                    \"obs\": \"9:1\",\n",
    "                                    \"error\": None\n",
    "                                }\n",
    "                            \n",
    "\n",
    "                            try:                                \n",
    "                                clear()\n",
    "                                # torch.set_float32_matmul_precision('high')\n",
    "                                \n",
    "                                _model_ = torch.compile(model(dataset.encoded_labels.shape[1], dataset.previous_encoded_labels.shape[1]))\n",
    "                                _lossfunction_ = loss_function[\"function\"](**{func:params[0](*params[1:]) for func,params in loss_function[\"function_params\"].items()})\n",
    "                                _optimizer_ = optim(_model_.parameters(), lr=learning_rate, **optim_params)\n",
    "\n",
    "\n",
    "                                # ---- Run ----\n",
    "                                result = Train_Test(\n",
    "                                    model=_model_,\n",
    "                                    loss_fn=_lossfunction_,\n",
    "                                    optimizer=_optimizer_,\n",
    "                                    epochs=epochs,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    batch_size=batch_size,\n",
    "                                    train_data=dataset,\n",
    "                                    test_data=dataset.get_test(),\n",
    "                                    id=time_id+\"_\"+str(id),\n",
    "                                    )\n",
    "                                    \n",
    "                                current[\"end_time\"] = time.time()\n",
    "                                current[\"best_epoch\"] = result[\"epoch\"]\n",
    "                                current[\"train_acc_best_epoch\"] = result[\"train_acc\"]\n",
    "                                current[\"train_loss_best_epoch\"] = result[\"train_loss\"]\n",
    "                                current[\"test_acc_best_epoch\"] = result[\"test_acc\"]\n",
    "                                current[\"test_loss_best_epoch\"] = result[\"test_loss\"]\n",
    "\n",
    "                                \n",
    "                                # torch.save(_model_.state_dict(), \"/media/stark/Models/Gustavo/\"+level+\"/\"+time_id+\"_\"+str(id)+\"_\"+current[\"model\"]+\".pth\")\n",
    "                                clear()                                \n",
    "                                \n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                                current[\"error\"] = str(e)\n",
    "                            \n",
    "                            results.append(current)\n",
    "                            pd.DataFrame(results).to_csv(\"./results/summarized/\"+str(time_id)+\"_models_train_test_\"+str(len(results))+\".csv\")\n",
    "                            \n",
    "                            id = id+1\n",
    "\n",
    "clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gustavo_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
