{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation_bernoulli(seq, prob=0.005):\n",
    "    idx = torch.bernoulli(prob * torch.ones(len(seq))).nonzero().squeeze(dim=1)\n",
    "    s = list(seq)\n",
    "\n",
    "    for i in idx.tolist():\n",
    "        s[i] = \"N\"\n",
    "\n",
    "    return \"\".join(s)\n",
    "\n",
    "def sequences_augmentation(data, level, cat, n):\n",
    "    to_copy = data.loc[data[level] == cat]\n",
    "\n",
    "    new_data = to_copy[0:1]\n",
    "    new_data = new_data.drop(new_data.index[0])\n",
    "\n",
    "    while new_data.shape[0] < n:\n",
    "        qnt = ((n-(new_data.shape[0])) / to_copy.shape[0]).__ceil__()\n",
    "\n",
    "        new_data = pd.concat(([to_copy]*qnt)+[new_data])\n",
    "        new_data[\"truncated_sequence\"] = new_data[\"truncated_sequence\"].apply(augmentation_bernoulli, prob=0.002)\n",
    "        new_data = new_data.drop_duplicates(subset=[\"truncated_sequence\"])\n",
    "    \n",
    "    new_data = new_data[:n-to_copy.shape[0]]\n",
    "    return new_data\n",
    "\n",
    "def data_augmentation(data, level, lower, upper):\n",
    "    class_count = data.groupby(level)[level].count().reset_index(name=\"count\")\n",
    "    \n",
    "    cats = class_count.loc[(class_count[\"count\"] < upper) & (class_count[\"count\"] >= lower)][level].to_list()\n",
    "\n",
    "    clones = sequences_augmentation(data, level, cats[0], upper)\n",
    "    for cat in cats[1:]:\n",
    "        clones = pd.concat([clones, sequences_augmentation(data, level, cat, upper)])\n",
    "\n",
    "    return pd.concat([data, clones])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_map = {\n",
    "    \"A\":[1.0, 0.0, 0.0, 0.0],\n",
    "    \"T\":[0.0, 1.0, 0.0, 0.0],\n",
    "    \"G\":[0.0, 0.0, 1.0, 0.0],\n",
    "    \"C\":[0.0, 0.0, 0.0, 1.0],\n",
    "\n",
    "    'W':[0.5, 0.5, 0.0, 0.0],\n",
    "    'S':[0.0, 0.0, 0.5, 0.5],\n",
    "    'M':[0.5, 0.0, 0.0, 0.5],\n",
    "    'K':[0.0, 0.5, 0.5, 0.0],\n",
    "    'R':[0.5, 0.0, 0.5, 0.0],\n",
    "    'Y':[0.0, 0.5, 0.0, 0.5],\n",
    "    \n",
    "    'B':[0.0, 0.3, 0.3, 0.3],\n",
    "    'D':[0.3, 0.3, 0.3, 0.0],\n",
    "    'H':[0.3, 0.3, 0.0, 0.3],\n",
    "    'V':[0.3, 0.0, 0.3, 0.3],\n",
    "\n",
    "    'N':[0.25, 0.25, 0.25, 0.25],\n",
    "}\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    encoded_seq = []\n",
    "\n",
    "    for base in sequence:\n",
    "        encoded_seq.append(base_map[base])\n",
    "    \n",
    "    return torch.tensor(encoded_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch dataset object to load Sequences and Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, train, test, level, augmentation=False):\n",
    "\n",
    "        self.classes = pd.concat([train[level], test[level]]).unique().tolist()\n",
    "        self.classes.sort()\n",
    "        self.level = level\n",
    "\n",
    "        if augmentation:\n",
    "            train = data_augmentation(train, level, 10, 500)\n",
    "        \n",
    "        self.labels = train[level]\n",
    "        self.encoded_labels = SequenceDataset.__encoded_labels__(self.classes, self.labels)\n",
    "        self.sequences = SequenceDataset.__sequences__(train)\n",
    "\n",
    "        self.test = SequenceDatasetTest(\n",
    "            labels = test[level],\n",
    "            classes = self.classes,\n",
    "            encoded_labels = SequenceDataset.__encoded_labels__(self.classes, test[level]),\n",
    "            sequences = SequenceDataset.__sequences__(test)\n",
    "            )\n",
    "\n",
    "    def __encoded_labels__(classes, labels):\n",
    "        return torch.nn.functional.one_hot(torch.tensor([classes.index(l) for l in labels]), len(classes)).type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    def __sequences__(ds):\n",
    "        sequences = []\n",
    "        for _, row in ds.iterrows():\n",
    "            sequences.append(encode_sequence(row[\"truncated_sequence\"]))        \n",
    "        return torch.stack(sequences, dim=0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return   self.sequences[idx], self.encoded_labels[idx]\n",
    "    \n",
    "    def __getitems__(self, ids):\n",
    "        idx = torch.tensor(ids, device=torch.device('cuda:0'))\n",
    "        return   list(zip(torch.index_select(self.sequences, 0, idx), torch.index_select(self.encoded_labels, 0, idx)))\n",
    "    \n",
    "    def get_test(self):\n",
    "        return self.test\n",
    "\n",
    "class SequenceDatasetTest(SequenceDataset):    \n",
    "    def __init__(self, labels, classes, encoded_labels, sequences):\n",
    "        self.labels = labels\n",
    "        self.classes = classes\n",
    "        self.encoded_labels = encoded_labels\n",
    "        self.sequences = sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate PyTorch DataLoader objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaders_generator(ds_train, ds_test, bs = 128):\n",
    "    train_loader = DataLoader(ds_train, batch_size=bs, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "    test_loader = DataLoader(ds_test, batch_size=bs, shuffle=True, generator=torch.Generator(device='cuda'))\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple CNN Model with 3 Conv1d layers and 2 fully connected layers\n",
    "# Input (1-Dimension 4-Channels)\n",
    "\n",
    "## Notes:\n",
    "#   - Bad fully connected size\n",
    "#   - Too much VRAM\n",
    "class SimplestCNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "\n",
    "        self.padding2 = nn.CircularPad1d((1,2))\n",
    "        self.conv2 = nn.Conv1d(8, 32, kernel_size=4)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.padding3 = nn.CircularPad1d((1,2))\n",
    "        self.conv3 = nn.Conv1d(32, 128, kernel_size=4)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28800, 28800*2)\n",
    "        self.linear2 = nn.Linear(28800*2, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.adAvgPool1(x)\n",
    "\n",
    "        x = self.conv2(self.padding2(x))\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        x = self.conv3(self.padding3(x))\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test of a CNN Model with 3 different Conv1d layers concatenated and 2 fully connected layers\n",
    "# Input (1-Dimension 4-Channels)\n",
    "\n",
    "## Notes:\n",
    "#   - \n",
    "class SimpleCNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimpleCNNClassifier, self).__init__()\n",
    "\n",
    "        self.padding = nn.CircularPad1d((1,2))\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(4, 4, kernel_size=4, groups=4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(4, 4, kernel_size=4)        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.act3 = nn.ReLU()        \n",
    "        self.conv3 = nn.Conv1d(4, 8, kernel_size=4, groups=4, dilation=2, padding=3, padding_mode=\"circular\")\n",
    "        \n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(14400, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        a = self.conv1(self.padding(x))\n",
    "        a = self.act1(a)        \n",
    "        \n",
    "        b = self.conv2(self.padding(x))\n",
    "        b = self.act2(b)\n",
    "        \n",
    "        c = self.conv3(x)\n",
    "        c = self.act3(c)\n",
    "        \n",
    "        x = torch.flatten(torch.cat([a,b,c], dim=1), 1)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        x = self.act4(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A test of a CNN Model with 3 different Conv1d layers concatenated, 2 fully connected layers, and dropouts\n",
    "# Input (1-Dimension 4-Channels)\n",
    "\n",
    "## Notes:\n",
    "#   - \n",
    "class SimpleCNNWithDropoutClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimpleCNNWithDropoutClassifier, self).__init__()\n",
    "\n",
    "        self.padding = nn.CircularPad1d((1,2))\n",
    "\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.conv1 = nn.Conv1d(4, 4, kernel_size=4, groups=4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(4, 4, kernel_size=4)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.act3 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv1d(4, 8, kernel_size=4, groups=4, dilation=2, padding=3, padding_mode=\"circular\")\n",
    "\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.linear1 = nn.Linear(14400, 7200)\n",
    "\n",
    "        self.dropout3 = nn.Dropout(p=0.2)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        x = self.dropout1(x)\n",
    "        a = self.conv1(self.padding(x))\n",
    "        a = self.act1(a)\n",
    "        \n",
    "        b = self.conv2(self.padding(x))\n",
    "        b = self.act2(b)\n",
    "        \n",
    "        c = self.conv3(x)\n",
    "        c = self.act3(c)\n",
    "        \n",
    "        x = torch.flatten(torch.cat([a,b,c], dim=1), 1)\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        \n",
    "        x = self.dropout3(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A CNN Model with 2 different Conv1d layers concatenated, 2 fully connected layers\n",
    "# Input (1-Dimension 4-Channels)\n",
    "\n",
    "## Notes:\n",
    "#   - Miss sequential Conv layers\n",
    "class BaseCNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(BaseCNNClassifier, self).__init__()\n",
    "\n",
    "        self.conv1_1 = nn.Conv1d(1, 4, kernel_size=4)\n",
    "        self.conv1_2 = nn.Conv1d(1, 4, kernel_size=4, dilation=2)\n",
    "        self.avgPool = nn.AvgPool1d(4, stride=2)\n",
    "        \n",
    "        self.padding = nn.CircularPad1d((1,2))\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(14392, 14392)\n",
    "        self.linear2 = nn.Linear(14392, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.unsqueeze(torch.flatten(x, start_dim=1), 1)\n",
    "        x = self.padding(x)\n",
    "        \n",
    "        x_1_1 = self.conv1_1(x)\n",
    "        x_1_2 = self.conv1_2(self.padding(x))      \n",
    "\n",
    "        x = torch.cat([x_1_1, x_1_2], dim=1)\n",
    "        x = self.avgPool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.act1(x)\n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        x = self.act2(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A UNet Model with 2 encode+decode levels and 2 fully connected layers\n",
    "# Input (1-Dimension 4-Channels)\n",
    "\n",
    "## Notes:\n",
    "#   - \n",
    "class UnetBasedCNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(UnetBasedCNNClassifier, self).__init__()\n",
    "\n",
    "        # First Encode Level\n",
    "        self.padding_e_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_1_1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "        self.act_e_1_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_e_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_1_2 = nn.Conv1d(8, 8, kernel_size=4)\n",
    "        self.act_e_1_2 = nn.ReLU()\n",
    "\n",
    "        self.avgPool_e_1_1 = nn.AvgPool1d(2, stride=2)\n",
    "\n",
    "\n",
    "        # Second Encode Level        \n",
    "        self.padding_e_2_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_2_1 = nn.Conv1d(8, 16, kernel_size=4)\n",
    "        self.act_e_2_1 = nn.ReLU()\n",
    "        \n",
    "        self.padding_e_2_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_2_2 = nn.Conv1d(16, 16, kernel_size=4)\n",
    "        self.act_e_2_2 = nn.ReLU()\n",
    "\n",
    "        self.avgPool_e_2_1 = nn.AvgPool1d(2, stride=2)\n",
    "\n",
    "        \n",
    "        # Transition Level\n",
    "        self.padding_t_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_t_1_1 = nn.Conv1d(16, 32, kernel_size = 4)\n",
    "        self.act_t_1_1 = nn.ReLU()\n",
    "        \n",
    "        self.padding_t_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_t_1_2 = nn.Conv1d(32, 32, kernel_size = 4)\n",
    "        self.act_t_1_2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        # First Decode Level\n",
    "        self.upconv_1_1 = nn.ConvTranspose1d(32, 16, kernel_size=2, stride=2)\n",
    "\n",
    "        self.padding_d_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_1_1 = nn.Conv1d(32, 16, kernel_size=4)\n",
    "        self.act_d_1_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_d_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_1_2 = nn.Conv1d(16, 16, kernel_size=4)\n",
    "        self.act_d_1_2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        # Second Decode Level\n",
    "        self.upconv_2_1 = nn.ConvTranspose1d(16, 8, kernel_size=2, stride=2)\n",
    "\n",
    "        self.padding_d_2_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_2_1 = nn.Conv1d(16, 8, kernel_size=4)\n",
    "        self.act_d_2_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_d_2_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_2_2 = nn.Conv1d(8, 8, kernel_size=4)\n",
    "        self.act_d_2_2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        # Output Level\n",
    "        self.conv_out_1 = nn.Conv1d(8, nClasses, kernel_size=1)\n",
    "        \n",
    "        self.linear_out_1 = nn.Linear(7200, 14400)\n",
    "        self.act_out_1 = nn.ReLU()\n",
    "        self.linear_out_2 = nn.Linear(14400, nClasses)\n",
    "        self.act_out_2 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.reshape(x.shape[0],4,-1)              # Reshape X shape: torch.Size([3, 4, 900])\n",
    "\n",
    "        # Encoder 1\n",
    "        x_e_1 = self.padding_e_1_1(x)               # Padding X shape: torch.Size([3, 4, 900])\n",
    "        x_e_1 = self.conv_e_1_1(x_e_1)              # Conv X shape: torch.Size([3, 8, 900])\n",
    "        x_e_1 = self.act_e_1_1(x_e_1)               # ActFunc X shape: torch.Size([3, 8, 900])\n",
    "        \n",
    "        x_e_1 = self.padding_e_1_2(x_e_1)           # Padding X shape: torch.Size([3, 8, 903])\n",
    "        x_e_1 = self.conv_e_1_2(x_e_1)              # Conv X shape: torch.Size([3, 8, 900])\n",
    "        x_e_1 = self.act_e_1_2(x_e_1)               # ActFunc X shape: torch.Size([3, 8, 900])\n",
    "        \n",
    "        x_e_p_1 = self.avgPool_e_1_1(x_e_1)         # Pool X shape: torch.Size([3, 8, 450])\n",
    "\n",
    "\n",
    "        # Encoder 2\n",
    "        x_e_2 = self.padding_e_2_1(x_e_p_1)         # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_e_2 = self.conv_e_2_1(x_e_2)              # Conv X shape: torch.Size([3, 16, 450])\n",
    "        x_e_2 = self.act_e_2_1(x_e_2)               # ActFunc X shape: torch.Size([3, 16, 450])\n",
    "        \n",
    "        x_e_2 = self.padding_e_2_2(x_e_2)           # Padding X shape: torch.Size([3, 16, 453])\n",
    "        x_e_2 = self.conv_e_2_2(x_e_2)              # Conv X shape: torch.Size([3, 16, 450])\n",
    "        x_e_2 = self.act_e_2_2(x_e_2)               # ActFunc X shape: torch.Size([3, 16, 450])\n",
    "\n",
    "        x_e_p_2 = self.avgPool_e_2_1(x_e_2)         # Pool X shape: torch.Size([3, 16, 225])\n",
    "\n",
    "\n",
    "        # Transition\n",
    "        x_t_1 = self.padding_t_1_1(x_e_p_2)         # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_t_1 = self.conv_t_1_1(x_t_1)              # Conv X shape: torch.Size([3, 32, 109])\n",
    "        x_t_1 = self.act_t_1_1(x_t_1)               # ActFunc X shape: torch.Size([3, 32, 109])\n",
    "        \n",
    "        x_t_1 = self.padding_t_1_2(x_t_1)           # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_t_1 = self.conv_t_1_2(x_t_1)              # Conv X shape: torch.Size([3, 32, 106])\n",
    "        x_t_1 = self.act_t_1_2(x_t_1)               # ActFunc X shape: torch.Size([3, 32, 106])\n",
    "\n",
    "\n",
    "        # Decode 1\n",
    "        x_d_1 = self.upconv_1_1(x_t_1)              # UpConv X shape: torch.Size([3, 16, 214])\n",
    "\n",
    "        x_d_1 = torch.cat([x_d_1, x_e_2], dim=1)    # \n",
    "\n",
    "        x_d_1 = self.padding_e_1_1(x_d_1)           # \n",
    "        x_d_1 = self.conv_d_1_1(x_d_1)              # \n",
    "        x_d_1 = self.act_d_1_1(x_d_1)               # \n",
    "\n",
    "        x_d_1 = self.padding_e_1_1(x_d_1)           # \n",
    "        x_d_1 = self.conv_d_1_2(x_d_1)              # \n",
    "        x_d_1 = self.act_d_1_2(x_d_1)               # \n",
    "\n",
    "\n",
    "        # Decode 2\n",
    "        x_d_2 = self.upconv_2_1(x_d_1)              # UpConv X shape: torch.Size([3, 16, 214])\n",
    "\n",
    "        x_d_2 = torch.cat([x_d_2, x_e_1], dim=1)    # \n",
    "\n",
    "        x_d_2 = self.padding_e_1_1(x_d_2)           # \n",
    "        x_d_2 = self.conv_d_2_1(x_d_2)              # \n",
    "        x_d_2 = self.act_d_2_1(x_d_2)               # \n",
    "\n",
    "        x_d_2 = self.padding_e_1_1(x_d_2)           # \n",
    "        x_d_2 = self.conv_d_2_2(x_d_2)              # \n",
    "        x_d_2 = self.act_d_2_2(x_d_2)               # \n",
    "\n",
    "\n",
    "        # Output\n",
    "\n",
    "        x = torch.flatten(x_d_2, 1)\n",
    "        \n",
    "        x = self.act_out_1(x)\n",
    "        x = self.linear_out_1(x)\n",
    "        \n",
    "        x = self.act_out_2(x)\n",
    "        x = self.linear_out_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A UNet Model variant with 2 encode+decode levels, 2 fully connected layers, and dropouts\n",
    "# Input (1-Dimension 4-Channels)\n",
    "\n",
    "## Notes:\n",
    "#   - \n",
    "class UnetBasedCNNWithDropoutClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(UnetBasedCNNWithDropoutClassifier, self).__init__()\n",
    "\n",
    "        self.input_dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        # First Encode Level\n",
    "        self.padding_e_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_1_1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "        self.act_e_1_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_e_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_1_2 = nn.Conv1d(8, 8, kernel_size=4)\n",
    "        self.act_e_1_2 = nn.ReLU()\n",
    "\n",
    "        self.avgPool_e_1_1 = nn.AvgPool1d(2, stride=2)\n",
    "\n",
    "\n",
    "        # Second Encode Level        \n",
    "        self.padding_e_2_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_2_1 = nn.Conv1d(8, 16, kernel_size=4)\n",
    "        self.act_e_2_1 = nn.ReLU()\n",
    "        \n",
    "        self.padding_e_2_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_2_2 = nn.Conv1d(16, 16, kernel_size=4)\n",
    "        self.act_e_2_2 = nn.ReLU()\n",
    "\n",
    "        self.avgPool_e_2_1 = nn.AvgPool1d(2, stride=2)\n",
    "\n",
    "        \n",
    "        # Transition Level\n",
    "        self.padding_t_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_t_1_1 = nn.Conv1d(16, 32, kernel_size = 4)\n",
    "        self.act_t_1_1 = nn.ReLU()\n",
    "        \n",
    "        self.padding_t_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_t_1_2 = nn.Conv1d(32, 32, kernel_size = 4)\n",
    "        self.act_t_1_2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        # First Decode Level\n",
    "        self.upconv_1_1 = nn.ConvTranspose1d(32, 16, kernel_size=2, stride=2)\n",
    "\n",
    "        self.padding_d_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_1_1 = nn.Conv1d(32, 16, kernel_size=4)\n",
    "        self.act_d_1_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_d_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_1_2 = nn.Conv1d(16, 16, kernel_size=4)\n",
    "        self.act_d_1_2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        # Second Decode Level\n",
    "        self.upconv_2_1 = nn.ConvTranspose1d(16, 8, kernel_size=2, stride=2)\n",
    "\n",
    "        self.padding_d_2_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_2_1 = nn.Conv1d(16, 8, kernel_size=4)\n",
    "        self.act_d_2_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_d_2_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_2_2 = nn.Conv1d(8, 8, kernel_size=4)\n",
    "        self.act_d_2_2 = nn.ReLU()\n",
    "\n",
    "\n",
    "        # Output Level\n",
    "        self.conv_out_1 = nn.Conv1d(8, nClasses, kernel_size=1)\n",
    "        \n",
    "        \n",
    "        self.output_dropout1 = nn.Dropout(p=0.2)\n",
    "        self.linear_out_1 = nn.Linear(7200, 14400)\n",
    "        self.act_out_1 = nn.ReLU()\n",
    "\n",
    "        \n",
    "        self.output_dropout2 = nn.Dropout(p=0.2)\n",
    "        self.linear_out_2 = nn.Linear(14400, nClasses)\n",
    "        self.act_out_2 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.reshape(x.shape[0],4,-1)              # Reshape X shape: torch.Size([3, 4, 900])\n",
    "        x = self.input_dropout1(x)\n",
    "\n",
    "        x_e_1 = self.padding_e_1_1(x)               # Padding X shape: torch.Size([3, 4, 900])\n",
    "        x_e_1 = self.conv_e_1_1(x_e_1)              # Conv X shape: torch.Size([3, 8, 900])\n",
    "        x_e_1 = self.act_e_1_1(x_e_1)               # ActFunc X shape: torch.Size([3, 8, 900])\n",
    "        \n",
    "        x_e_1 = self.padding_e_1_2(x_e_1)           # Padding X shape: torch.Size([3, 8, 903])\n",
    "        x_e_1 = self.conv_e_1_2(x_e_1)              # Conv X shape: torch.Size([3, 8, 900])\n",
    "        x_e_1 = self.act_e_1_2(x_e_1)               # ActFunc X shape: torch.Size([3, 8, 900])\n",
    "        \n",
    "        x_e_p_1 = self.avgPool_e_1_1(x_e_1)         # Pool X shape: torch.Size([3, 8, 450])\n",
    "\n",
    "\n",
    "        x_e_2 = self.padding_e_2_1(x_e_p_1)         # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_e_2 = self.conv_e_2_1(x_e_2)              # Conv X shape: torch.Size([3, 16, 450])\n",
    "        x_e_2 = self.act_e_2_1(x_e_2)               # ActFunc X shape: torch.Size([3, 16, 450])\n",
    "        \n",
    "        x_e_2 = self.padding_e_2_2(x_e_2)           # Padding X shape: torch.Size([3, 16, 453])\n",
    "        x_e_2 = self.conv_e_2_2(x_e_2)              # Conv X shape: torch.Size([3, 16, 450])\n",
    "        x_e_2 = self.act_e_2_2(x_e_2)               # ActFunc X shape: torch.Size([3, 16, 450])\n",
    "\n",
    "        x_e_p_2 = self.avgPool_e_2_1(x_e_2)         # Pool X shape: torch.Size([3, 16, 225])\n",
    "\n",
    "\n",
    "        x_t_1 = self.padding_t_1_1(x_e_p_2)         # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_t_1 = self.conv_t_1_1(x_t_1)              # Conv X shape: torch.Size([3, 32, 109])\n",
    "        x_t_1 = self.act_t_1_1(x_t_1)               # ActFunc X shape: torch.Size([3, 32, 109])\n",
    "        \n",
    "        x_t_1 = self.padding_t_1_2(x_t_1)           # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_t_1 = self.conv_t_1_2(x_t_1)              # Conv X shape: torch.Size([3, 32, 106])\n",
    "        x_t_1 = self.act_t_1_2(x_t_1)               # ActFunc X shape: torch.Size([3, 32, 106])\n",
    "\n",
    "\n",
    "        x_d_1 = self.upconv_1_1(x_t_1)              # UpConv X shape: torch.Size([3, 16, 214])\n",
    "\n",
    "        x_d_1 = torch.cat([x_d_1, x_e_2], dim=1)    # \n",
    "\n",
    "        x_d_1 = self.padding_e_1_1(x_d_1)           # \n",
    "        x_d_1 = self.conv_d_1_1(x_d_1)              # \n",
    "        x_d_1 = self.act_d_1_1(x_d_1)               # \n",
    "\n",
    "        x_d_1 = self.padding_e_1_1(x_d_1)           # \n",
    "        x_d_1 = self.conv_d_1_2(x_d_1)              # \n",
    "        x_d_1 = self.act_d_1_2(x_d_1)               # \n",
    "\n",
    "\n",
    "        x_d_2 = self.upconv_2_1(x_d_1)              # UpConv X shape: torch.Size([3, 16, 214])\n",
    "\n",
    "        x_d_2 = torch.cat([x_d_2, x_e_1], dim=1)    # \n",
    "\n",
    "        x_d_2 = self.padding_e_1_1(x_d_2)           # \n",
    "        x_d_2 = self.conv_d_2_1(x_d_2)              # \n",
    "        x_d_2 = self.act_d_2_1(x_d_2)               # \n",
    "\n",
    "        x_d_2 = self.padding_e_1_1(x_d_2)           # \n",
    "        x_d_2 = self.conv_d_2_2(x_d_2)              # \n",
    "        x_d_2 = self.act_d_2_2(x_d_2)               # \n",
    "\n",
    "\n",
    "        x = torch.flatten(x_d_2, 1)\n",
    "        x = self.output_dropout1(x)\n",
    "        \n",
    "        x = self.act_out_1(x)\n",
    "        x = self.linear_out_1(x)\n",
    "        \n",
    "        x = self.output_dropout2(x)\n",
    "        x = self.act_out_2(x)\n",
    "        x = self.linear_out_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A UNet Model with 2 encode(with dilation)+decode levels and 2 fully connected layers\n",
    "# Input (1-Dimension 4-Channels)\n",
    "\n",
    "## Notes:\n",
    "#   - \n",
    "class UnetBasedCNNWithDilationClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(UnetBasedCNNWithDilationClassifier, self).__init__()\n",
    "\n",
    "        # First Encode Level\n",
    "        self.padding_e_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_1_1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "        self.act_e_1_1 = nn.ReLU()\n",
    "        \n",
    "        self.convd_e_1_1 = nn.Conv1d(4, 8, kernel_size=4, groups=4, dilation=2, padding=3, padding_mode=\"circular\")\n",
    "        self.actd_e_1_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_e_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_1_2 = nn.Conv1d(16, 16, kernel_size=4)\n",
    "        self.act_e_1_2 = nn.ReLU()\n",
    "\n",
    "        self.avgPool_e_1_1 = nn.AvgPool1d(2, stride=2)\n",
    "\n",
    "        # Second Encode Level        \n",
    "        self.padding_e_2_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_2_1 = nn.Conv1d(16, 32, kernel_size=4)\n",
    "        self.act_e_2_1 = nn.ReLU()\n",
    "        \n",
    "        self.convd_e_2_1 = nn.Conv1d(16, 32, kernel_size=4, groups=4, dilation=2, padding=3, padding_mode=\"circular\")\n",
    "        self.actd_e_2_1 = nn.ReLU()\n",
    "        \n",
    "        self.padding_e_2_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_2_2 = nn.Conv1d(64, 64, kernel_size=4)\n",
    "        self.act_e_2_2 = nn.ReLU()\n",
    "\n",
    "        self.avgPool_e_2_1 = nn.AvgPool1d(2, stride=2)\n",
    "        \n",
    "        # Transition Level\n",
    "        self.padding_t_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_t_1_1 = nn.Conv1d(64, 128, kernel_size = 4)\n",
    "        self.act_t_1_1 = nn.ReLU()\n",
    "        \n",
    "        self.padding_t_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_t_1_2 = nn.Conv1d(128, 128, kernel_size = 4)\n",
    "        self.act_t_1_2 = nn.ReLU()\n",
    "\n",
    "        # First Decode Level\n",
    "        self.upconv_1_1 = nn.ConvTranspose1d(128, 64, kernel_size=2, stride=2)\n",
    "\n",
    "        self.padding_d_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_1_1 = nn.Conv1d(128, 64, kernel_size=4)\n",
    "        self.act_d_1_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_d_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_1_2 = nn.Conv1d(64, 64, kernel_size=4)\n",
    "        self.act_d_1_2 = nn.ReLU()\n",
    "\n",
    "        # Second Decode Level\n",
    "        self.upconv_2_1 = nn.ConvTranspose1d(64, 32, kernel_size=2, stride=2)\n",
    "\n",
    "        self.padding_d_2_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_2_1 = nn.Conv1d(48, 24, kernel_size=4)\n",
    "        self.act_d_2_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_d_2_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_2_2 = nn.Conv1d(24, 24, kernel_size=4)\n",
    "        self.act_d_2_2 = nn.ReLU()\n",
    "\n",
    "        # Output Level\n",
    "        self.conv_out_1 = nn.Conv1d(24, nClasses, kernel_size=1)\n",
    "        \n",
    "        self.linear_out_1 = nn.Linear(21600, 7200)\n",
    "        self.act_out_1 = nn.ReLU()\n",
    "\n",
    "        self.linear_out_2 = nn.Linear(7200, nClasses)\n",
    "        self.act_out_2 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.reshape(x.shape[0],4,-1)              # Reshape X shape: torch.Size([3, 4, 900])\n",
    "\n",
    "        # Encoder 1\n",
    "        x_e_1 = self.padding_e_1_1(x)               # Padding X shape: torch.Size([3, 4, 900])\n",
    "        x_e_1 = self.conv_e_1_1(x_e_1)              # Conv X shape: torch.Size([3, 8, 900])        \n",
    "        x_e_1 = self.act_e_1_1(x_e_1)               # ActFunc X shape: torch.Size([3, 8, 900])        \n",
    "\n",
    "        x_e_1_d = self.convd_e_1_1(x)\n",
    "        x_e_1_d = self.actd_e_1_1(x_e_1_d)\n",
    "        \n",
    "        x_e_1 = torch.cat([x_e_1, x_e_1_d], dim=1)\n",
    "        \n",
    "        x_e_1 = self.padding_e_1_2(x_e_1)           # Padding X shape: torch.Size([3, 8, 903])\n",
    "        x_e_1 = self.conv_e_1_2(x_e_1)              # Conv X shape: torch.Size([3, 8, 900])\n",
    "        x_e_1 = self.act_e_1_2(x_e_1)               # ActFunc X shape: torch.Size([3, 8, 900])\n",
    "        \n",
    "        x_e_p_1 = self.avgPool_e_1_1(x_e_1)         # Pool X shape: torch.Size([3, 8, 450])\n",
    "\n",
    "        # Encoder 2\n",
    "        x_e_2 = self.padding_e_2_1(x_e_p_1)         # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_e_2 = self.conv_e_2_1(x_e_2)              # Conv X shape: torch.Size([3, 16, 450])\n",
    "        x_e_2 = self.act_e_2_1(x_e_2)               # ActFunc X shape: torch.Size([3, 16, 450])\n",
    "\n",
    "        x_e_2_d = self.convd_e_2_1(x_e_p_1)\n",
    "        x_e_2_d = self.actd_e_2_1(x_e_2_d)        \n",
    "        x_e_2 = torch.cat([x_e_2, x_e_2_d], dim=1)\n",
    "        \n",
    "        x_e_2 = self.padding_e_2_2(x_e_2)           # Padding X shape: torch.Size([3, 16, 453])\n",
    "        x_e_2 = self.conv_e_2_2(x_e_2)              # Conv X shape: torch.Size([3, 16, 450])\n",
    "        x_e_2 = self.act_e_2_2(x_e_2)               # ActFunc X shape: torch.Size([3, 16, 450])\n",
    "\n",
    "        x_e_p_2 = self.avgPool_e_2_1(x_e_2)         # Pool X shape: torch.Size([3, 16, 225])\n",
    "\n",
    "\n",
    "        # Transition\n",
    "        x_t_1 = self.padding_t_1_1(x_e_p_2)         # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_t_1 = self.conv_t_1_1(x_t_1)              # Conv X shape: torch.Size([3, 32, 109])\n",
    "        x_t_1 = self.act_t_1_1(x_t_1)               # ActFunc X shape: torch.Size([3, 32, 109])\n",
    "        \n",
    "        x_t_1 = self.padding_t_1_2(x_t_1)           # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_t_1 = self.conv_t_1_2(x_t_1)              # Conv X shape: torch.Size([3, 32, 106])\n",
    "        x_t_1 = self.act_t_1_2(x_t_1)               # ActFunc X shape: torch.Size([3, 32, 106])\n",
    "\n",
    "\n",
    "        # Decode 1\n",
    "        x_d_1 = self.upconv_1_1(x_t_1)              # UpConv X shape: torch.Size([3, 16, 214])\n",
    "\n",
    "        x_d_1 = torch.cat([x_d_1, x_e_2], dim=1)    # \n",
    "\n",
    "        x_d_1 = self.padding_e_1_1(x_d_1)           # \n",
    "        x_d_1 = self.conv_d_1_1(x_d_1)              # \n",
    "        x_d_1 = self.act_d_1_1(x_d_1)               # \n",
    "\n",
    "        x_d_1 = self.padding_e_1_1(x_d_1)           # \n",
    "        x_d_1 = self.conv_d_1_2(x_d_1)              # \n",
    "        x_d_1 = self.act_d_1_2(x_d_1)               # \n",
    "\n",
    "\n",
    "        # Decode 2\n",
    "        x_d_2 = self.upconv_2_1(x_d_1)              # UpConv X shape: torch.Size([3, 16, 214])\n",
    "\n",
    "        x_d_2 = torch.cat([x_d_2, x_e_1], dim=1)    # \n",
    "\n",
    "        x_d_2 = self.padding_e_1_1(x_d_2)           # \n",
    "        x_d_2 = self.conv_d_2_1(x_d_2)              # \n",
    "        x_d_2 = self.act_d_2_1(x_d_2)               # \n",
    "\n",
    "        x_d_2 = self.padding_e_1_1(x_d_2)           # \n",
    "        x_d_2 = self.conv_d_2_2(x_d_2)              # \n",
    "        x_d_2 = self.act_d_2_2(x_d_2)               # \n",
    "\n",
    "\n",
    "        # Output\n",
    "        x = torch.flatten(x_d_2, 1)\n",
    "\n",
    "        x = self.act_out_1(x)\n",
    "        x = self.linear_out_1(x)\n",
    "        \n",
    "        x = self.act_out_2(x)\n",
    "        x = self.linear_out_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A UNet Model variant with 2 encode(with dilation)+decode levels, 2 fully connected layers, and dropouts\n",
    "# Input (1-Dimension 4-Channels)\n",
    "\n",
    "## Notes:\n",
    "#   - \n",
    "class UnetBasedCNNWithDropoutAndDilationClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(UnetBasedCNNWithDropoutAndDilationClassifier, self).__init__()\n",
    "\n",
    "        self.input_dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        # First Encode Level\n",
    "        self.padding_e_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_1_1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "        self.act_e_1_1 = nn.ReLU()\n",
    "        \n",
    "        self.convd_e_1_1 = nn.Conv1d(4, 8, kernel_size=4, groups=4, dilation=2, padding=3, padding_mode=\"circular\")\n",
    "        self.actd_e_1_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_e_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_1_2 = nn.Conv1d(16, 16, kernel_size=4)\n",
    "        self.act_e_1_2 = nn.ReLU()\n",
    "\n",
    "        self.avgPool_e_1_1 = nn.AvgPool1d(2, stride=2)\n",
    "\n",
    "        # Second Encode Level        \n",
    "        self.padding_e_2_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_2_1 = nn.Conv1d(16, 32, kernel_size=4)\n",
    "        self.act_e_2_1 = nn.ReLU()\n",
    "        \n",
    "        self.convd_e_2_1 = nn.Conv1d(16, 32, kernel_size=4, groups=4, dilation=2, padding=3, padding_mode=\"circular\")\n",
    "        self.actd_e_2_1 = nn.ReLU()\n",
    "        \n",
    "        self.padding_e_2_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_e_2_2 = nn.Conv1d(64, 64, kernel_size=4)\n",
    "        self.act_e_2_2 = nn.ReLU()\n",
    "\n",
    "        self.avgPool_e_2_1 = nn.AvgPool1d(2, stride=2)\n",
    "\n",
    "        # Transition Level\n",
    "        self.padding_t_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_t_1_1 = nn.Conv1d(64, 128, kernel_size = 4)\n",
    "        self.act_t_1_1 = nn.ReLU()\n",
    "        \n",
    "        self.padding_t_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_t_1_2 = nn.Conv1d(128, 128, kernel_size = 4)\n",
    "        self.act_t_1_2 = nn.ReLU()\n",
    "\n",
    "        # First Decode Level\n",
    "        self.upconv_1_1 = nn.ConvTranspose1d(128, 64, kernel_size=2, stride=2)\n",
    "\n",
    "        self.padding_d_1_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_1_1 = nn.Conv1d(128, 64, kernel_size=4)\n",
    "        self.act_d_1_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_d_1_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_1_2 = nn.Conv1d(64, 64, kernel_size=4)\n",
    "        self.act_d_1_2 = nn.ReLU()\n",
    "\n",
    "        # Second Decode Level\n",
    "        self.upconv_2_1 = nn.ConvTranspose1d(64, 32, kernel_size=2, stride=2)\n",
    "\n",
    "        self.padding_d_2_1 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_2_1 = nn.Conv1d(48, 24, kernel_size=4)\n",
    "        self.act_d_2_1 = nn.ReLU()\n",
    "\n",
    "        self.padding_d_2_2 = nn.CircularPad1d((1,2))\n",
    "        self.conv_d_2_2 = nn.Conv1d(24, 24, kernel_size=4)\n",
    "        self.act_d_2_2 = nn.ReLU()\n",
    "\n",
    "        # Output Level\n",
    "        self.conv_out_1 = nn.Conv1d(24, nClasses, kernel_size=1)\n",
    "        \n",
    "        self.output_dropout1 = nn.Dropout(p=0.2)\n",
    "        self.linear_out_1 = nn.Linear(21600, 43200)\n",
    "        self.act_out_1 = nn.ReLU()\n",
    "\n",
    "        self.output_dropout2 = nn.Dropout(p=0.2)\n",
    "        self.linear_out_2 = nn.Linear(43200, nClasses)\n",
    "        self.act_out_2 = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0],4,-1)              # Reshape X shape: torch.Size([3, 4, 900])\n",
    "\n",
    "        x = self.input_dropout1(x)\n",
    "\n",
    "        # Encode 1\n",
    "        x_e_1 = self.padding_e_1_1(x)               # Padding X shape: torch.Size([3, 4, 900])\n",
    "        x_e_1 = self.conv_e_1_1(x_e_1)              # Conv X shape: torch.Size([3, 8, 900])        \n",
    "        x_e_1 = self.act_e_1_1(x_e_1)               # ActFunc X shape: torch.Size([3, 8, 900])        \n",
    "\n",
    "        x_e_1_d = self.convd_e_1_1(x)\n",
    "        x_e_1_d = self.actd_e_1_1(x_e_1_d)\n",
    "        \n",
    "        x_e_1 = torch.cat([x_e_1, x_e_1_d], dim=1)\n",
    "        \n",
    "        x_e_1 = self.padding_e_1_2(x_e_1)           # Padding X shape: torch.Size([3, 8, 903])\n",
    "        x_e_1 = self.conv_e_1_2(x_e_1)              # Conv X shape: torch.Size([3, 8, 900])\n",
    "        x_e_1 = self.act_e_1_2(x_e_1)               # ActFunc X shape: torch.Size([3, 8, 900])\n",
    "        \n",
    "        x_e_p_1 = self.avgPool_e_1_1(x_e_1)         # Pool X shape: torch.Size([3, 8, 450])\n",
    "\n",
    "        # Encode 2\n",
    "        x_e_2 = self.padding_e_2_1(x_e_p_1)         # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_e_2 = self.conv_e_2_1(x_e_2)              # Conv X shape: torch.Size([3, 16, 450])\n",
    "        x_e_2 = self.act_e_2_1(x_e_2)               # ActFunc X shape: torch.Size([3, 16, 450])\n",
    "\n",
    "        x_e_2_d = self.convd_e_2_1(x_e_p_1)\n",
    "        x_e_2_d = self.actd_e_2_1(x_e_2_d)        \n",
    "        x_e_2 = torch.cat([x_e_2, x_e_2_d], dim=1)\n",
    "        \n",
    "        x_e_2 = self.padding_e_2_2(x_e_2)           # Padding X shape: torch.Size([3, 16, 453])\n",
    "        x_e_2 = self.conv_e_2_2(x_e_2)              # Conv X shape: torch.Size([3, 16, 450])\n",
    "        x_e_2 = self.act_e_2_2(x_e_2)               # ActFunc X shape: torch.Size([3, 16, 450])\n",
    "\n",
    "        x_e_p_2 = self.avgPool_e_2_1(x_e_2)         # Pool X shape: torch.Size([3, 16, 225])\n",
    "\n",
    "        # Transition\n",
    "        x_t_1 = self.padding_t_1_1(x_e_p_2)         # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_t_1 = self.conv_t_1_1(x_t_1)              # Conv X shape: torch.Size([3, 32, 109])\n",
    "        x_t_1 = self.act_t_1_1(x_t_1)               # ActFunc X shape: torch.Size([3, 32, 109])\n",
    "        \n",
    "        x_t_1 = self.padding_t_1_2(x_t_1)           # Padding X shape: torch.Size([3, 8, 453])\n",
    "        x_t_1 = self.conv_t_1_2(x_t_1)              # Conv X shape: torch.Size([3, 32, 106])\n",
    "        x_t_1 = self.act_t_1_2(x_t_1)               # ActFunc X shape: torch.Size([3, 32, 106])\n",
    "\n",
    "        # Decode 1\n",
    "        x_d_1 = self.upconv_1_1(x_t_1)              # UpConv X shape: torch.Size([3, 16, 214])\n",
    "\n",
    "        x_d_1 = torch.cat([x_d_1, x_e_2], dim=1)    # \n",
    "\n",
    "        x_d_1 = self.padding_e_1_1(x_d_1)           # \n",
    "        x_d_1 = self.conv_d_1_1(x_d_1)              # \n",
    "        x_d_1 = self.act_d_1_1(x_d_1)               # \n",
    "\n",
    "        x_d_1 = self.padding_e_1_1(x_d_1)           # \n",
    "        x_d_1 = self.conv_d_1_2(x_d_1)              # \n",
    "        x_d_1 = self.act_d_1_2(x_d_1)               # \n",
    "\n",
    "        # Decode 2\n",
    "        x_d_2 = self.upconv_2_1(x_d_1)              # UpConv X shape: torch.Size([3, 16, 214])\n",
    "\n",
    "        x_d_2 = torch.cat([x_d_2, x_e_1], dim=1)    # \n",
    "\n",
    "        x_d_2 = self.padding_e_1_1(x_d_2)           # \n",
    "        x_d_2 = self.conv_d_2_1(x_d_2)              # \n",
    "        x_d_2 = self.act_d_2_1(x_d_2)               # \n",
    "\n",
    "        x_d_2 = self.padding_e_1_1(x_d_2)           # \n",
    "        x_d_2 = self.conv_d_2_2(x_d_2)              # \n",
    "        x_d_2 = self.act_d_2_2(x_d_2)               # \n",
    "\n",
    "        # Output\n",
    "        x = torch.flatten(x_d_2, 1)\n",
    "\n",
    "        x = self.output_dropout1(x)\n",
    "        x = self.act_out_1(x)\n",
    "        x = self.linear_out_1(x)\n",
    "        \n",
    "        x = self.output_dropout2(x)\n",
    "        x = self.act_out_2(x)\n",
    "        x = self.linear_out_2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary models tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "\n",
    "        self.padding2 = nn.CircularPad1d((1,2))\n",
    "        self.conv2 = nn.Conv1d(8, 32, kernel_size=4)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.padding3 = nn.CircularPad1d((1,2))\n",
    "        self.conv3 = nn.Conv1d(32, 128, kernel_size=4)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28800, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.adAvgPool1(x)\n",
    "\n",
    "        x = self.conv2(self.padding2(x))\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        x = self.conv3(self.padding3(x))\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0_1layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0_1layer, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 32, kernel_size=3)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28832, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "      \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0_1layerk2(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0_1layerk2, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 32, kernel_size=2)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28864, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "      \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0_1layerk4(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0_1layerk4, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 32, kernel_size=4)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28800, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "      \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0_1layer16(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0_1layer16, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 16, kernel_size=3)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(14416, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "      \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0_1layerGELU(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0_1layerGELU, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 32, kernel_size=3)\n",
    "        \n",
    "        self.act1 = nn.GELU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28832, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "      \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0_1layer64c(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0_1layer64c, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 64, kernel_size=3)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(57664, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "      \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0_1layerPooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0_1layerPooling, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 32, kernel_size=3)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(14400, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.adAvgPool1(x)\n",
    "      \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier0_1layer64cPooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier0_1layer64cPooling, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 64, kernel_size=3)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28800, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.adAvgPool1(x)\n",
    "      \n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_2layers(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_2layers, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 16, kernel_size=4)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "\n",
    "        self.padding2 = nn.CircularPad1d((1,2))\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=4)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(7200, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.adAvgPool1(x)\n",
    "\n",
    "        x = self.conv2(self.padding2(x))\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_2layers_concat(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_2layers_concat, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 16, kernel_size=4)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "\n",
    "        self.padding2 = nn.CircularPad1d((1,2))\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=4)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.act4 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(14400, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x1 = self.adAvgPool1(x)\n",
    "\n",
    "        x = self.conv2(self.padding2(x1))\n",
    "        x2 = self.adAvgPool2(x)\n",
    "        \n",
    "        # x = torch.flatten(x, 1)\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        x2 = torch.flatten(x2, 1)\n",
    "\n",
    "        x = torch.cat([x1,x2], dim=1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Padding to maintain input size\n",
    "        self.padding = nn.CircularPad1d((1,2))\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Store the input for the residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # Main path\n",
    "        out = self.padding(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        residual = self.shortcut(residual)\n",
    "        \n",
    "        # Add residual connection\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SimplestCNNClassifier_2layers_Residual(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_2layers_Residual, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(7200, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlockGELU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4):\n",
    "        super(ResidualBlockGELU, self).__init__()\n",
    "        \n",
    "        # Padding to maintain input size\n",
    "        self.padding = nn.CircularPad1d((1,2))\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Activation\n",
    "        self.act = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Store the input for the residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # Main path\n",
    "        out = self.padding(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        residual = self.shortcut(residual)\n",
    "        \n",
    "        # Add residual connection\n",
    "        out += residual\n",
    "        out = self.act(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SimplestCNNClassifier_2layers_ResidualGELU(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_2layers_ResidualGELU, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlockGELU(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlockGELU(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(7200, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class SimplestCNNClassifier_GELU2layers_Residual(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_GELU2layers_Residual, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(7200, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_3layers_Residual(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_3layers_Residual, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        self.residual_block3 = ResidualBlock(32, 64)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(112)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(14400, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Third residual block\n",
    "        x = self.residual_block3(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_4layers_Residual_Pooling(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_4layers_Residual_Pooling, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        # Two additional residual blocks\n",
    "        self.residual_block3 = ResidualBlock(32, 64)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(112)\n",
    "        \n",
    "        self.residual_block4 = ResidualBlock(64, 128)\n",
    "        self.adAvgPool4 = nn.AdaptiveAvgPool1d(56)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # Note: You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(7168, 7168)\n",
    "        self.linear2 = nn.Linear(7168, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Third residual block\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        # Fourth residual block\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.adAvgPool4(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_GELU_4layers_Residual_Pooling(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_GELU_4layers_Residual_Pooling, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        # Two additional residual blocks\n",
    "        self.residual_block3 = ResidualBlock(32, 64)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(112)\n",
    "        \n",
    "        self.residual_block4 = ResidualBlock(64, 128)\n",
    "        self.adAvgPool4 = nn.AdaptiveAvgPool1d(56)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # Note: You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(7168, 7168)\n",
    "        self.linear2 = nn.Linear(7168, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Third residual block\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        # Fourth residual block\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.adAvgPool4(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_4layers_Residual(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_4layers_Residual, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        # self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        # self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        # Two additional residual blocks\n",
    "        self.residual_block3 = ResidualBlock(32, 64)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block4 = ResidualBlock(64, 128)\n",
    "        self.adAvgPool4 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # Note: You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(28800, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        # x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        # x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Third residual block\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        # Fourth residual block\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.adAvgPool4(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_6layers_Residual(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_6layers_Residual, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        self.residual_block3 = ResidualBlock(32, 64)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(112)\n",
    "        \n",
    "        self.residual_block4 = ResidualBlock(64, 128)\n",
    "        self.adAvgPool4 = nn.AdaptiveAvgPool1d(56)\n",
    "        \n",
    "        # Two additional residual blocks\n",
    "        self.residual_block5 = ResidualBlock(128, 256)\n",
    "        self.adAvgPool5 = nn.AdaptiveAvgPool1d(28)\n",
    "        \n",
    "        self.residual_block6 = ResidualBlock(256, 512)\n",
    "        self.adAvgPool6 = nn.AdaptiveAvgPool1d(14)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # Note: You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(7168, 7168)\n",
    "        self.linear2 = nn.Linear(7168, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Third residual block\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        # Fourth residual block\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.adAvgPool4(x)\n",
    "        \n",
    "        # Fifth residual block\n",
    "        x = self.residual_block5(x)\n",
    "        x = self.adAvgPool5(x)\n",
    "        \n",
    "        # Sixth residual block\n",
    "        x = self.residual_block6(x)\n",
    "        x = self.adAvgPool6(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_6layers_Residual2(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_6layers_Residual2, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling starting from 600 and reducing\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(600)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(400)\n",
    "        \n",
    "        self.residual_block3 = ResidualBlock(32, 64)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(266)\n",
    "        \n",
    "        self.residual_block4 = ResidualBlock(64, 128)\n",
    "        self.adAvgPool4 = nn.AdaptiveAvgPool1d(177)\n",
    "        \n",
    "        self.residual_block5 = ResidualBlock(128, 256)\n",
    "        self.adAvgPool5 = nn.AdaptiveAvgPool1d(118)\n",
    "        \n",
    "        self.residual_block6 = ResidualBlock(256, 512)\n",
    "        self.adAvgPool6 = nn.AdaptiveAvgPool1d(78)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # Note: You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(39936, 7168)\n",
    "        self.linear2 = nn.Linear(7168, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Third residual block\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        # Fourth residual block\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.adAvgPool4(x)\n",
    "        \n",
    "        # Fifth residual block\n",
    "        x = self.residual_block5(x)\n",
    "        x = self.adAvgPool5(x)\n",
    "        \n",
    "        # Sixth residual block\n",
    "        x = self.residual_block6(x)\n",
    "        x = self.adAvgPool6(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier_8layers_Residual(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_8layers_Residual, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock(4, 16)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock(16, 32)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        self.residual_block3 = ResidualBlock(32, 64)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(112)\n",
    "        \n",
    "        self.residual_block4 = ResidualBlock(64, 128)\n",
    "        self.adAvgPool4 = nn.AdaptiveAvgPool1d(56)\n",
    "        \n",
    "        self.residual_block5 = ResidualBlock(128, 256)\n",
    "        self.adAvgPool5 = nn.AdaptiveAvgPool1d(28)\n",
    "        \n",
    "        self.residual_block6 = ResidualBlock(256, 512)\n",
    "        self.adAvgPool6 = nn.AdaptiveAvgPool1d(14)\n",
    "        \n",
    "        # Two additional residual blocks\n",
    "        self.residual_block7 = ResidualBlock(512, 1024)\n",
    "        self.adAvgPool7 = nn.AdaptiveAvgPool1d(7)\n",
    "        \n",
    "        self.residual_block8 = ResidualBlock(1024, 2048)\n",
    "        self.adAvgPool8 = nn.AdaptiveAvgPool1d(3)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # Note: You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(6144, 6144)\n",
    "        self.linear2 = nn.Linear(6144, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Third residual block\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        # Fourth residual block\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.adAvgPool4(x)\n",
    "        \n",
    "        # Fifth residual block\n",
    "        x = self.residual_block5(x)\n",
    "        x = self.adAvgPool5(x)\n",
    "        \n",
    "        # Sixth residual block\n",
    "        x = self.residual_block6(x)\n",
    "        x = self.adAvgPool6(x)\n",
    "        \n",
    "        # Seventh residual block\n",
    "        x = self.residual_block7(x)\n",
    "        x = self.adAvgPool7(x)\n",
    "        \n",
    "        # Eighth residual block\n",
    "        x = self.residual_block8(x)\n",
    "        x = self.adAvgPool8(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock3k(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(ResidualBlock3k, self).__init__()\n",
    "        \n",
    "        # Padding to maintain input size\n",
    "        # With kernel_size 3, we need asymmetric padding\n",
    "        self.padding = nn.CircularPad1d((1,1))\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Store the input for the residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # Main path\n",
    "        out = self.padding(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn1(out)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        residual = self.shortcut(residual)\n",
    "        \n",
    "        # Add residual connection\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SimplestCNNClassifier_8layers_Residual3k(nn.Module):\n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier_8layers_Residual3k, self).__init__()\n",
    "        \n",
    "        # Residual blocks with adaptive pooling\n",
    "        self.residual_block1 = ResidualBlock3k(4, 16, kernel_size=3)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "        \n",
    "        self.residual_block2 = ResidualBlock3k(16, 32, kernel_size=3)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "        \n",
    "        self.residual_block3 = ResidualBlock3k(32, 64, kernel_size=3)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(112)\n",
    "        \n",
    "        self.residual_block4 = ResidualBlock3k(64, 128, kernel_size=3)\n",
    "        self.adAvgPool4 = nn.AdaptiveAvgPool1d(56)\n",
    "        \n",
    "        self.residual_block5 = ResidualBlock3k(128, 256, kernel_size=3)\n",
    "        self.adAvgPool5 = nn.AdaptiveAvgPool1d(28)\n",
    "        \n",
    "        self.residual_block6 = ResidualBlock3k(256, 512, kernel_size=3)\n",
    "        self.adAvgPool6 = nn.AdaptiveAvgPool1d(14)\n",
    "        \n",
    "        self.residual_block7 = ResidualBlock3k(512, 1024, kernel_size=3)\n",
    "        self.adAvgPool7 = nn.AdaptiveAvgPool1d(7)\n",
    "        \n",
    "        self.residual_block8 = ResidualBlock3k(1024, 2048, kernel_size=3)\n",
    "        self.adAvgPool8 = nn.AdaptiveAvgPool1d(3)\n",
    "        \n",
    "        # Activation and fully connected layers\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        # Calculate the input size for linear layers\n",
    "        # Note: You might need to adjust this based on your specific input dimensions\n",
    "        self.linear1 = nn.Linear(6144, 6144)\n",
    "        self.linear2 = nn.Linear(6144, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Move channel dimension\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        \n",
    "        # First residual block\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.adAvgPool1(x)\n",
    "        \n",
    "        # Second residual block\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        # Third residual block\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        # Fourth residual block\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.adAvgPool4(x)\n",
    "        \n",
    "        # Fifth residual block\n",
    "        x = self.residual_block5(x)\n",
    "        x = self.adAvgPool5(x)\n",
    "        \n",
    "        # Sixth residual block\n",
    "        x = self.residual_block6(x)\n",
    "        x = self.adAvgPool6(x)\n",
    "        \n",
    "        # Seventh residual block\n",
    "        x = self.residual_block7(x)\n",
    "        x = self.adAvgPool7(x)\n",
    "        \n",
    "        # Eighth residual block\n",
    "        x = self.residual_block8(x)\n",
    "        x = self.adAvgPool8(x)\n",
    "        \n",
    "        # Flatten and fully connected layers\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier1(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier1, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "\n",
    "        self.padding2 = nn.CircularPad1d((1,2))\n",
    "        self.conv2 = nn.Conv1d(8, 32, kernel_size=4)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.padding3 = nn.CircularPad1d((1,2))\n",
    "        self.conv3 = nn.Conv1d(32, 128, kernel_size=4)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.linear1 = nn.Linear(28800, 14400)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(14400, 7200)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.adAvgPool1(x)\n",
    "\n",
    "        x = self.conv2(self.padding2(x))\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        x = self.conv3(self.padding3(x))\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act5(x)\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier2(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier2, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # Input: (batch_size, 1, 4, 900)\n",
    "        self.padding1 = nn.CircularPad2d((1, 2, 1, 1))\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 4), stride=1)\n",
    "        # Output: (batch_size, 8, 900, 4)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.padding2 = nn.CircularPad2d((1, 2, 1, 1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 4), stride=1)\n",
    "        # Output: (batch_size, 16, 4, 900)\n",
    "        \n",
    "        # Third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 4), stride=1, padding=(1, 0), padding_mode=\"circular\")\n",
    "        # Output: (batch_size, 32, 1, 900)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear((32 * 1 * 900 ), (nClasses*2))\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear((nClasses*2), nClasses)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.padding1(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "\n",
    "        x = self.padding2(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        x = self.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(self.dropout1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier3(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier3, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 8, kernel_size=4)\n",
    "\n",
    "        self.padding2 = nn.CircularPad1d((1,2))\n",
    "        self.conv2 = nn.Conv1d(8, 16, kernel_size=4)\n",
    "\n",
    "        self.padding3 = nn.CircularPad1d((1,2))\n",
    "        self.conv3 = nn.Conv1d(16, 32, kernel_size=4)\n",
    "\n",
    "        self.linear1 = nn.Linear(28800, 7200)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.conv2(self.padding2(x))        \n",
    "        x = self.conv3(self.padding3(x))\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier4(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier4, self).__init__()\n",
    "\n",
    "        self.padding1 = nn.CircularPad1d((1,2))\n",
    "        self.conv1 = nn.Conv1d(4, 8, kernel_size=3)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool1d(450)\n",
    "\n",
    "        self.padding2 = nn.CircularPad1d((1,2))\n",
    "        self.conv2 = nn.Conv1d(8, 32, kernel_size=3)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.padding3 = nn.CircularPad1d((1,2))\n",
    "        self.conv3 = nn.Conv1d(32, 128, kernel_size=3)\n",
    "        self.adAvgPool3 = nn.AdaptiveAvgPool1d(225)\n",
    "\n",
    "        self.act4 = nn.GELU()\n",
    "\n",
    "        self.linear1 = nn.Linear(28800, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        x = self.conv1(self.padding1(x))\n",
    "        x = self.adAvgPool1(x)\n",
    "\n",
    "        x = self.conv2(self.padding2(x))\n",
    "        x = self.adAvgPool2(x)\n",
    "        \n",
    "        x = self.conv3(self.padding3(x))\n",
    "        x = self.adAvgPool3(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier5(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier5, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # Input: (batch_size, 1, 4, 900)\n",
    "        self.padding1 = nn.CircularPad2d((1, 2, 1, 1))\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3, 4), stride=1)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool2d(output_size=(450, 4))\n",
    "        # Output: (batch_size, 8, 450, 4)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.padding2 = nn.CircularPad2d((1, 2, 1, 1))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 4), stride=1)\n",
    "        self.adAvgPool2 = nn.AdaptiveAvgPool2d(output_size=(225, 4))\n",
    "        # Output: (batch_size, 16, 4, 900)\n",
    "        \n",
    "        # Third convolutional layer\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 4), stride=1, padding=(1, 0), padding_mode=\"circular\")\n",
    "        # Output: (batch_size, 32, 1, 900)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(64800, 7200)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(7200, nClasses)\n",
    "        \n",
    "        # Activation function\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x1 = self.padding1(x)\n",
    "        x1 = self.act(self.conv1(x1))\n",
    "        # print(x1.shape)\n",
    "        x = self.adAvgPool1(x1)\n",
    "        # print(x1.shape)\n",
    "\n",
    "        x2 = self.padding2(x)\n",
    "        x2 = self.act(self.conv2(x2))\n",
    "        # print(x2.shape)\n",
    "        x = self.adAvgPool2(x2)\n",
    "        # print(x2.shape)\n",
    "        \n",
    "        x = self.act(self.conv3(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "\n",
    "        # x = torch.cat([x, x1, x2], dim=1)\n",
    "        # print(\"cat\")\n",
    "        \n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        # print(x1.shape)\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "        # print(x2.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = torch.cat([x,x1,x2], dim=1)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Flatten\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.act(self.fc1(self.dropout1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier5_1layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier5_1layer, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # Input: (batch_size, 1, 4, 900)\n",
    "        self.padding1 = nn.CircularPad2d((1, 2, 1, 1))\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 4), stride=1)\n",
    "        # Output: (batch_size, 8, 450, 4)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(115200, 7200)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(7200, nClasses)\n",
    "        \n",
    "        # Activation function\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.padding1(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.act(self.fc1(self.dropout1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier5_1layer64c(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier5_1layer64c, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # Input: (batch_size, 1, 4, 900)\n",
    "        self.padding1 = nn.CircularPad2d((1, 2, 1, 1))\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 4), stride=1)\n",
    "        # Output: (batch_size, 8, 450, 4)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(230400, 7200)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(7200, nClasses)\n",
    "        \n",
    "        # Activation function\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.padding1(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.act(self.fc1(self.dropout1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier5_1layerPooling(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier5_1layerPooling, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # Input: (batch_size, 1, 4, 900)\n",
    "        self.padding1 = nn.CircularPad2d((1, 2, 1, 1))\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 4), stride=1)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool2d(output_size=(450, 4))\n",
    "        # Output: (batch_size, 8, 450, 4)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(57600, 7200)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(7200, nClasses)\n",
    "        \n",
    "        # Activation function\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.padding1(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "        x = self.adAvgPool1(x)\n",
    "        # print(x1.shape)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.act(self.fc1(self.dropout1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestCNNClassifier5_1layerPooling64c(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimplestCNNClassifier5_1layerPooling64c, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # Input: (batch_size, 1, 4, 900)\n",
    "        self.padding1 = nn.CircularPad2d((1, 2, 1, 1))\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 4), stride=1)\n",
    "        self.adAvgPool1 = nn.AdaptiveAvgPool2d(output_size=(450, 4))\n",
    "        # Output: (batch_size, 8, 450, 4)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(115200, 7200)\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(7200, nClasses)\n",
    "        \n",
    "        # Activation function\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x,1)\n",
    "\n",
    "        # Convolutional layers\n",
    "        x = self.padding1(x)\n",
    "        x = self.act(self.conv1(x))\n",
    "        # print(x.shape)\n",
    "        x = self.adAvgPool1(x)\n",
    "        # print(x1.shape)\n",
    "\n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.act(self.fc1(self.dropout1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNNClassifier1(nn.Module):\n",
    "    \n",
    "    def __init__(self, nClasses):\n",
    "        super(SimpleCNNClassifier1, self).__init__()\n",
    "\n",
    "        self.padding = nn.CircularPad1d((1,2))\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(4, 4, kernel_size=4, groups=4)\n",
    "        self.act1 = nn.ReLU()\n",
    "        \n",
    "        self.conv1_1 = nn.Conv1d(4, 8, kernel_size=4, groups=4, dilation=2, padding=3, padding_mode=\"circular\")\n",
    "        self.act1_1 = nn.ReLU()        \n",
    "        \n",
    "        self.conv2 = nn.Conv1d(12, 24, kernel_size=4)        \n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(24, 32, kernel_size=4)        \n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv1d(32, 64, kernel_size=4)        \n",
    "        self.act4 = nn.ReLU()        \n",
    "        \n",
    "        self.act5 = nn.ReLU()\n",
    "\n",
    "        self.linear1 = nn.Linear(57600, 7200)\n",
    "        self.linear2 = nn.Linear(7200, nClasses)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.movedim(x, -1, -2)\n",
    "\n",
    "        a = self.conv1(self.padding(x))\n",
    "        a = self.act1(a)        \n",
    "        # print(a.shape)\n",
    "        \n",
    "        b = self.conv1_1((x))\n",
    "        b = self.act1_1(b)\n",
    "        # print(b.shape)\n",
    "        \n",
    "        x = torch.cat([a,b], dim=1)\n",
    "        # print(x.shape)\n",
    "\n",
    "        \n",
    "        x = self.conv2(self.padding(x))\n",
    "        x = self.act2(x)        \n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.conv3(self.padding(x))\n",
    "        x = self.act3(x)        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = self.conv4(self.padding(x))\n",
    "        x = self.act4(x)        \n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(x.shape)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        \n",
    "        x = self.act5(x)\n",
    "        \n",
    "        x = self.linear2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\n",
    "    \"class\", \n",
    "    \"order\", \n",
    "    \"family\", \n",
    "    \"genus\",\n",
    "    \"species\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [\n",
    "    # 64,\n",
    "    # 128,\n",
    "    # 256,\n",
    "    # 512,\n",
    "    # 2048,\n",
    "    # 10000,\n",
    "    \"dynamic\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [\n",
    "    # 1,\n",
    "    # 2,\n",
    "    # 5,\n",
    "    # 20,\n",
    "    # 50,\n",
    "    # 100,\n",
    "    # 150,\n",
    "    # 200,\n",
    "    # 300,\n",
    "    # 500,\n",
    "    # 600,\n",
    "    1000,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [\n",
    "\n",
    "    # SimplestCNNClassifier_6layers_Residual,\n",
    "    # SimplestCNNClassifier_6layers_Residual2,\n",
    "    SimplestCNNClassifier_8layers_Residual,\n",
    "    # SimplestCNNClassifier_8layers_Residual3k,\n",
    "\n",
    "    # SimplestCNNClassifier_GELU_4layers_Residual_Pooling,\n",
    "    # SimplestCNNClassifier_4layers_Residual,\n",
    "    # SimplestCNNClassifier_4layers_Residual_Pooling,\n",
    "    # SimplestCNNClassifier_2layers_ResidualGELU,\n",
    "    # SimplestCNNClassifier_GELU2layers_Residual,\n",
    "\n",
    "    # # SimplestCNNClassifier_2layers,\n",
    "    # SimplestCNNClassifier_2layers_Residual,\n",
    "    # # # SimplestCNNClassifier_2layers_concat,\n",
    "    # SimplestCNNClassifier_3layers_Residual,\n",
    "\n",
    "    # # # SimplestCNNClassifier0_1layer,\n",
    "    # # SimplestCNNClassifier0_1layerPooling,\n",
    "    # # # SimplestCNNClassifier0_1layerGELU,\n",
    "    # # # SimplestCNNClassifier0_1layer64c,\n",
    "    # # # SimplestCNNClassifier0_1layer64cPooling,\n",
    "    # # SimplestCNNClassifier5_1layer,\n",
    "    # # # # SimplestCNNClassifier5_1layer64c,\n",
    "    # # # # SimplestCNNClassifier5_1layerPooling,\n",
    "    # # # # SimplestCNNClassifier5_1layerPooling64c,\n",
    "\n",
    "    # # # SimplestCNNClassifier0_1layer16,\n",
    "    # # # SimplestCNNClassifier0_1layerk4,\n",
    "    # # SimplestCNNClassifier0_1layerk2,\n",
    "\n",
    "    # # SimplestCNNClassifier0,\n",
    "    # # # SimplestCNNClassifier1,\n",
    "    # # # SimplestCNNClassifier2,\n",
    "    # # # SimplestCNNClassifier3,\n",
    "    # # SimplestCNNClassifier5,\n",
    "    # # # SimpleCNNClassifier1,\n",
    "\n",
    "\n",
    "    # # # SimplestCNNClassifier,\n",
    "    # # # SimpleCNNClassifier,\n",
    "    # # # SimpleCNNWithDropoutClassifier,\n",
    "    # # # BaseCNNClassifier,\n",
    "    # # # UnetBasedCNNClassifier,\n",
    "    # # # UnetBasedCNNWithDropoutClassifier,\n",
    "    # # # UnetBasedCNNWithDilationClassifier,\n",
    "    # # # UnetBasedCNNWithDropoutAndDilationClassifier,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = {\n",
    "    \"CrossEntropyLoss\":{\n",
    "        \"function\":nn.CrossEntropyLoss,\n",
    "        \"params\":{},\n",
    "        \"function_params\":{}\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [\n",
    "    # 5e-2,\n",
    "    # 1e-2,\n",
    "    5e-3,\n",
    "    # 1e-3,\n",
    "    # 5e-4,\n",
    "    # 1e-4,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [\n",
    "    \n",
    "    {\n",
    "        \"optim\":torch.optim.AdamW,\n",
    "        \"params\":{\n",
    "            \"weight_decay\":1.0,\n",
    "            \"amsgrad\":True\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # {\n",
    "    #     \"optim\":torch.optim.AdamW,\n",
    "    #     \"params\":{\n",
    "    #         \"weight_decay\":1.0,\n",
    "    #         \"amsgrad\":False\n",
    "    #     }\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    #     \"optim\":torch.optim.AdamW,\n",
    "    #     \"params\":{\n",
    "    #         \"weight_decay\":5e-1,\n",
    "    #         \"amsgrad\":True\n",
    "    #     }\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    #     \"optim\":torch.optim.AdamW,\n",
    "    #     \"params\":{\n",
    "    #         \"weight_decay\":5e-1,\n",
    "    #         \"amsgrad\":False\n",
    "    #     }\n",
    "    # },\n",
    "\n",
    "    # {\n",
    "    #     \"optim\":torch.optim.AdamW,\n",
    "    #     \"params\":{\n",
    "    #         \"weight_decay\":1e-3,\n",
    "    #         \"amsgrad\":True\n",
    "    #     }\n",
    "    # },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparams = {\n",
    "    \"batch_size\": batch_sizes,\n",
    "    \"epochs\": epochs,\n",
    "    \"model\": models_list,\n",
    "    \"loss_function\": loss_functions,\n",
    "    \"learning_rate\": learning_rates,\n",
    "    \"optimizer\": optimizers    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Test(model, loss_fn, optimizer, epochs, learning_rate, batch_size, train_data,test_data,id=\"\"):\n",
    "    \n",
    "    print(\"Model: \\t\\t\\t\"+(model._get_name() if not model._get_name() == \"OptimizedModule\" else model.__dict__[\"_modules\"][\"_orig_mod\"].__class__.__name__))\n",
    "    print(\"  Loss Func.: \\t\\t\"+loss_fn._get_name())\n",
    "    print(\"  Optimizer: \\t\\t\"+type(optimizer).__name__)\n",
    "    print(\"  Epochs: \\t\\t\"+str(epochs))\n",
    "    print(\"  Learning Rate: \\t\"+str(learning_rate))\n",
    "\n",
    "    print(\"\\nModel Arch: \")\n",
    "    print(str(model))\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "    # Test CUDA compatibility\n",
    "    if torch.cuda.get_device_capability() < (7, 0):\n",
    "        print(\"Exiting because torch.compile is not supported on this device.\")\n",
    "        import sys\n",
    "        sys.exit(0)\n",
    "\n",
    "\n",
    "    epochs_results = []\n",
    "    current = {\n",
    "        \"model\":(model._get_name() if not model._get_name() == \"OptimizedModule\" else model.__dict__[\"_modules\"][\"_orig_mod\"].__class__.__name__),\n",
    "        \"loss_function\":loss_fn._get_name(),\n",
    "        \"epoch\":None,\n",
    "        \"learning_rate\":learning_rate,\n",
    "        \"batch_size\":None,\n",
    "        \"train_size\":None,\n",
    "        \"test_size\":None,\n",
    "        \"optimizer\":type(optimizer).__name__,\n",
    "        \"train_acc\":None,\n",
    "        \"train_loss\":None,\n",
    "        \"test_acc\":None,\n",
    "        \"test_loss\":None,\n",
    "    }\n",
    "\n",
    "    # Prepare batch sizes to use\n",
    "    if batch_size == \"dynamic\":\n",
    "        bss = [20000, 20000, 20000, 1000, 20000, 20000, 20000, 20000, 256, 20000, 20000, 20000, 20000, 128, 20000]\n",
    "    else:\n",
    "        bss = [batch_size]\n",
    "    if len(bss) > epochs:\n",
    "        bss = bss[0:epochs]\n",
    "    print(\"Batch Sizes List: \"+str(bss))\n",
    "    batch_lim = int(epochs/len(bss))\n",
    "    \n",
    "    \n",
    "    t_start = time.time()\n",
    "    best = {\n",
    "        \"epoch\":0,\n",
    "        \"train_acc\":0,\n",
    "        \"train_loss\":10000000,\n",
    "        \"test_acc\":0,\n",
    "        \"test_loss\":10000000,\n",
    "    }\n",
    "    \n",
    "    train_loader = None\n",
    "    test_loader = None\n",
    "    \n",
    "    \n",
    "    scaler = GradScaler(device=device)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,  # First restart\n",
    "        T_mult=2,  # Period multiplier\n",
    "        eta_min=1e-10,  # Minimum learning rate\n",
    "    )\n",
    "\n",
    "    # Epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "\n",
    "        # Create DataLoaders with current batch size\n",
    "        if epoch%batch_lim == 0 and len(bss) > 0:\n",
    "            if train_loader:\n",
    "                del train_loader\n",
    "            if test_loader:\n",
    "                del test_loader\n",
    "\n",
    "            batch_size = bss.pop(0)\n",
    "            train_loader, test_loader = loaders_generator(train_data, test_data, batch_size)\n",
    "\n",
    "        print(\"Batch Size: \"+str(batch_size))\n",
    "        \n",
    "        \n",
    "        # Train Phase\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "\n",
    "        # Run train over the batches\n",
    "        optimizer.zero_grad()\n",
    "        for batch, (X, y) in enumerate(train_loader):\n",
    "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "                # Compute prediction and loss\n",
    "                pred = model(X)\n",
    "                loss = loss_fn(pred, y)\n",
    "                \n",
    "            # Backpropagation\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Gradien clipping\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update the learning rate\n",
    "            scheduler.step(epoch + batch / len(train_loader))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update results\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "        # Train results\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader.dataset)\n",
    "        print(\"Last Learning Rate: \"+str(scheduler.get_last_lr()[0]))\n",
    "        print(f\"Train Error: \\n Accuracy: {(100*train_acc):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "        # Test Phase\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_loader:\n",
    "                pred = model(X)\n",
    "                test_loss += loss_fn(pred, y).item()\n",
    "                test_acc += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
    "\n",
    "        # Test results\n",
    "        test_loss /= len(test_loader)\n",
    "        test_acc /= len(test_loader.dataset)\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*test_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "        # Update Results\n",
    "        if best[\"test_acc\"] < test_acc or (best[\"test_acc\"] == test_acc and best[\"train_acc\"] < train_acc):\n",
    "            best[\"epoch\"] = epoch+1\n",
    "            best[\"test_acc\"] = test_acc\n",
    "            best[\"test_loss\"] = test_loss\n",
    "            best[\"train_acc\"] = train_acc\n",
    "            best[\"train_loss\"] = train_loss\n",
    "\n",
    "            # If accuracy over 50%, export the current best treined model\n",
    "            if test_acc > 0.5:\n",
    "                torch.save(model.state_dict(), \"/media/stark/Models/Gustavo/\"+train_data.level+\"/\"+str(id)+\"_\"+current[\"model\"]+\".pth\")\n",
    "                                \n",
    "                    \n",
    "        current[\"epoch\"] = epoch+1\n",
    "        current[\"batch_size\"] = batch_size\n",
    "        current[\"learning_rate\"] = scheduler.get_last_lr()[0]\n",
    "        current[\"train_size\"] = train_loader.dataset.__len__()\n",
    "        current[\"test_size\"] = test_loader.dataset.__len__()\n",
    "        current[\"train_acc\"] = train_acc\n",
    "        current[\"train_loss\"] = train_loss\n",
    "        current[\"test_acc\"] = test_acc\n",
    "        current[\"test_loss\"] = test_loss\n",
    "\n",
    "        epochs_results.append(current.copy())\n",
    "\n",
    "    # Save Train/Test iteration information\n",
    "    pd.DataFrame(epochs_results).to_csv(\"./results/epochs/\"+str(id)+\"__\"+current[\"model\"]+\"_train_test.csv\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    print(f\"Best Epoch:{best['epoch']} \\n\\tAccuracy: {(100*best['test_acc']):>0.1f}%, Avg loss: {best['test_loss']:>8f} \\n\")\n",
    "    print(\"Train and Test execution time: \"+str(format(time.time()-t_start, '.4f'))+\"s\")\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Global references\n",
    "_model_ = None\n",
    "_lossfunction_ = None\n",
    "_optimizer_ = None\n",
    "\n",
    "# Function to clean cache\n",
    "def clear():\n",
    "    global _model_, _lossfunction_, _optimizer_\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.compiler.reset()\n",
    "    torch._dynamo.reset()\n",
    "\n",
    "    if _model_:\n",
    "        del _model_\n",
    "        _model_ = None\n",
    "    if _lossfunction_:\n",
    "        del _lossfunction_\n",
    "        _lossfunction_ = None\n",
    "    if _optimizer_:\n",
    "        del _optimizer_\n",
    "        _optimizer_ = None\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "results = []\n",
    "current = {}\n",
    "\n",
    "id = 0\n",
    "time_id = str(int(time.time()))\n",
    "print(\"Time ID: \"+str(time_id))\n",
    "\n",
    "splitters = [\n",
    "    \"RandomSplit\", \n",
    "    \"StratifiedSplit\"\n",
    "    ]\n",
    "\n",
    "apply_augmentation = [\n",
    "    False,\n",
    "    True,\n",
    "]\n",
    "\n",
    "for mat_mul in [False, True]:\n",
    "    for augmentation in apply_augmentation:\n",
    "        for splitter in splitters:\n",
    "            for level in levels:\n",
    "                clear()\n",
    "\n",
    "                # Load train and test datasets\n",
    "                train_data = pd.read_csv(\"../new_data/\"+splitter+\"/\"+level+\"/train_dataset.csv\")#[0:1000]\n",
    "                test_data = pd.read_csv(\"../new_data/\"+splitter+\"/\"+level+\"/test_dataset.csv\")#[0:1000]\n",
    "                print(level)\n",
    "                print(train_data.shape)\n",
    "                print(test_data.shape)\n",
    "\n",
    "                dataset = SequenceDataset(\n",
    "                    train=train_data, \n",
    "                    test=test_data, \n",
    "                    level=level, \n",
    "                    augmentation=augmentation)\n",
    "                \n",
    "                print(dataset.__len__())\n",
    "                print(dataset.test.__len__())\n",
    "\n",
    "\n",
    "                for batch_size in hiperparams[\"batch_size\"]:\n",
    "                    for epochs in hiperparams[\"epochs\"]:\n",
    "                        for model in hiperparams[\"model\"]:\n",
    "                            for loss_function_name, loss_function in hiperparams[\"loss_function\"].items():\n",
    "                                for learning_rate in hiperparams[\"learning_rate\"]:\n",
    "                                    for optimizer in hiperparams[\"optimizer\"]:\n",
    "                                        clear()\n",
    "                                        \n",
    "                                        optim = optimizer[\"optim\"]\n",
    "                                        optim_params = optimizer[\"params\"] if \"params\" in optimizer.keys() else {}\n",
    "\n",
    "                                        current = {\n",
    "                                                \"id\": id,\n",
    "                                                \"start_time\":time.time(),\n",
    "                                                \"end_time\": None,\n",
    "                                                \"level\": level,\n",
    "                                                \"splitter\": splitter,\n",
    "                                                \"augmentation\": augmentation,\n",
    "                                                \"batch_size\": batch_size,\n",
    "                                                \"epochs\": epochs,\n",
    "                                                \"model\": model.__name__,\n",
    "                                                \"loss_function\": loss_function_name+\" (\"+str(loss_function[\"function\"])+\")\",\n",
    "                                                \"learning_rate\": learning_rate,\n",
    "                                                \"optimizer\": optim.__name__+\" (params: \"+str(optim_params)+\")\",\n",
    "                                                \"mat_mul\": mat_mul,\n",
    "                                                \"obs\": \"9:1\",\n",
    "                                                \"reserved_memory\": None,\n",
    "                                                \"error\": None\n",
    "                                            }\n",
    "\n",
    "\n",
    "                                        try:\n",
    "                                            # Change precision to improve model performance \n",
    "                                            if mat_mul:\n",
    "                                                torch.set_float32_matmul_precision('high')\n",
    "                                            else:\n",
    "                                                torch.set_float32_matmul_precision('highest')\n",
    "                                            \n",
    "                                            # Initialize a compiled model, loss function, and optimizer\n",
    "                                            _model_ = torch.compile(model(dataset.encoded_labels.shape[1]))\n",
    "                                            _lossfunction_ = loss_function[\"function\"](**{func:params[0](*params[1:]) for func,params in loss_function[\"function_params\"].items()})\n",
    "                                            _optimizer_ = optim(_model_.parameters(), lr=learning_rate, **optim_params)\n",
    "\n",
    "\n",
    "                                            # Runt Train-Test\n",
    "                                            result = Train_Test(\n",
    "                                                model=_model_,\n",
    "                                                loss_fn=_lossfunction_,\n",
    "                                                optimizer=_optimizer_,\n",
    "                                                epochs=epochs,\n",
    "                                                learning_rate=learning_rate,\n",
    "                                                batch_size=batch_size,\n",
    "                                                train_data=dataset,\n",
    "                                                test_data=dataset.get_test(),\n",
    "                                                id=time_id+\"_\"+str(id),\n",
    "                                                )\n",
    "                                                \n",
    "                                            current[\"end_time\"] = time.time()\n",
    "                                            current[\"best_epoch\"] = result[\"epoch\"]\n",
    "                                            current[\"train_acc_best_epoch\"] = result[\"train_acc\"]\n",
    "                                            current[\"train_loss_best_epoch\"] = result[\"train_loss\"]\n",
    "                                            current[\"test_acc_best_epoch\"] = result[\"test_acc\"]\n",
    "                                            current[\"test_loss_best_epoch\"] = result[\"test_loss\"]\n",
    "\n",
    "                                            current[\"reserved_memory\"] = torch.cuda.memory_reserved() / 1024 / 1024  # Convert to MB\n",
    "\n",
    "                                            clear()                                \n",
    "                                            \n",
    "                                        except Exception as e:\n",
    "                                            print(e)\n",
    "                                            current[\"error\"] = str(e)\n",
    "                                        \n",
    "                                        # Save the results\n",
    "                                        results.append(current)\n",
    "                                        pd.DataFrame(results).to_csv(\"./results/summarized/\"+str(time_id)+\"_models_train_test_\"+str(len(results))+\".csv\")\n",
    "                                        \n",
    "                                        id = id+1\n",
    "\n",
    "clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gustavo_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
